# AI Security Digest â€“ June 2025

A digest of AI security research, insights, reports, upcoming events, and tools & resources. Follow the [AI Security community](https://linktr.ee/AISECHUB) on [Twitter](https://twitter.com) and [LinkedIn group](https://linkedin.com) for additional updates.

Sponsored by [CyberBiz](https://cyber-biz.com/) and [InnovGuard.com](https://innovguard.com) â€“ Technology Risk & Cybersecurity Advisory, Innovate and Invest with Confidence, Lead with Assurance.

## ğŸ” Insights

ğŸ“Œ Mapping: MAESTRO Threatsâ€Šâ€”â€ŠMITRE D3FEND Techniquesâ€Šâ€”â€ŠThis website presents an interactive exploration of the intersection between two pivotal cybersecurity frameworks: MAESTRO and MITRE D3FEND. It aims to provide cybersecurity professionals with actionable insights into securing Agentic AI systems by mapping identified threats to corresponding defensive techniques, by Edward Lee. https://edward-playground.github.io/maestro-d3fend-mapping/

ğŸ“Œ 10 Key Risks of Shadow AIâ€Šâ€”â€ŠA practical breakdown of Shadow AI: how unmanaged AI useâ€Šâ€”â€Šincluding tools, models, and featuresâ€Šâ€”â€Šcreates hidden risks across security, compliance, data, and governance. https://www.linkedin.com/pulse/10-key-risks-shadow-ai-tal-eliyahu-9aopc/

ğŸ“Œ How an AI Agent Vulnerability in LangSmith Could Lead to Stolen API Keys and Hijacked LLM Responses - by Sasi Levi and Gal Moyal, Noma Security - https://noma.security/blog/how-an-ai-agent-vulnerability-in-langsmith-could-lead-to-stolen-api-keys-and-hijacked-llm-responses/

ğŸ“Œ Explore the latest threats to Model Context Protocol (MCP)â€Šâ€”â€Šcovering issues from prompt injection to agent hijackingâ€Šâ€”â€Šin this digest collected by Adversa AI. https://adversa.ai/blog/mcp-security-digest-june-2025/

ğŸ“Œ GenAI Guardrails: Implementation & Best Practicesâ€Šâ€”â€Š Lasso outlines how organizations are designing and deploying guardrails for generative AIâ€Šâ€”â€Šincluding challenges, frameworks, and real-world examples. https://www.lasso.security/blog/genai-guardrails

ğŸ“Œ Trend Microâ€™s â€œUnveiling AI Agent Vulnerabilitiesâ€ 4-part series explores key security threats in agentic AI systemsâ€Šâ€”â€Šincluding [Part I: Introduction](https://www.trendmicro.com/vinfo/us/security/news/threat-landscape/unveiling-ai-agent-vulnerabilities-part-i-introduction-to-ai-agent-vulnerabilities), [Part II: Code Execution](https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/unveiling-ai-agent-vulnerabilities-code-execution), [Part III: Data Exfiltration](https://www.trendmicro.com/vinfo/us/security/news/threat-landscape/unveiling-ai-agent-vulnerabilities-part-iii-data-exfiltration), and [Part IV: Database Access](https://www.trendmicro.com/vinfo/us/security/news/vulnerabilities-and-exploits/unveiling-ai-agent-vulnerabilities-part-iv-database-access-vulnerabilities).

ğŸ“Œ Malicious AI Models Undermine Software Supply-Chain Securityâ€Šâ€”â€Šhttps://cacm.acm.org/research/malicious-ai-models-undermine-software-supply-chain-security/

ğŸ“Œ Leaking Secrets in the Age of AIâ€Šâ€”â€Š Shay Berkovich and Rami McCarthy scanned public repos and found widespread AI-related secret leaksâ€Šâ€”â€Šdriven by notebooks, hardcoded configs, and gaps in todayâ€™s secret scanning tools. https://www.wiz.io/blog/leaking-ai-secrets-in-public-code

ğŸ“Œ What is AI Assets Sprawl? Causes, Risks, and Control Strategiesâ€Šâ€”â€Š Dor Sarig from Pillar Security explores how unmanaged AI models, prompts, and tools accumulate across enterprisesâ€Šâ€”â€Šcreating security, compliance, and visibility challenges without proper controls. https://www.pillar.security/blog/what-is-ai-assets-sprawl-causes-risks-and-control-strategies   

ğŸ“Œ Is your AI safe? Threat analysis of MCPâ€Šâ€”â€Š Nil Ashkenazi outlines how Model Context Protocol (MCP) introduces risks like tool misuse, prompt-based exfiltration, and unsafe server chaining. Focus is on real-world attack paths and how insecure integrations can be exploited. https://www.cyberark.com/resources/threat-research-blog/is-your-ai-safe-threat-analysis-of-mcp-model-context-protocol

ğŸ“Œ A New Identity Framework for AI Agentsâ€Š by Omar Santos â€”â€ŠWe are all experiencing the rapid proliferation of autonomous AI agents and Multi-Agent Systems (MAS). These are no longer AI chatbots and assistants; they are increasingly self-directed entities capable of making decisions, performing actions, and interacting with critical systems at unprecedented scales. We need to perform fundamental re-evaluations of how identities are managed and access is controlled for these AI agents. https://community.cisco.com/t5/security-blogs/a-new-identity-framework-for-ai-agents/ba-p/5294337

ğŸ“Œ Hunting Deserialization Vulnerabilities With Claudeâ€Šâ€”â€Š TrustedSec exploring how to find zero-days in .NET assemblies using Model Context Protocol (MCP). https://trustedsec.com/blog/hunting-deserialization-vulnerabilities-with-claude

ğŸ“Œ Uncovering Nytheon AI â€” Vitaly Simonovich ğŸ‡®ğŸ‡± from Cato Networks, analyzes Nytheon AI, a Tor-based GenAI platform built from jailbroken open-source models (Llama 3.2, Gemma, Qwen2), offering code generation, multilingual chat, image parsing, and API accessâ€Šâ€”â€Šwrapped in a modern SaaS-style interface. https://www.catonetworks.com/blog/cato-ctrl-nytheon-ai-a-new-platform-of-uncensored-llms/

ğŸ“Œ Touchpoints Between AI and Non-Human Identitiesâ€Šâ€”â€Š Tal Skverer from Astrix Security and Ophir Oren ğŸ‡®ğŸ‡± from Bayer examine how AI agents rely on non-human identities (NHIs)â€Šâ€”â€ŠAPI keys, service accounts, OAuth appsâ€Šâ€”â€Što operate across platforms. Unlike traditional automation, these agents request dynamic access, mimic users, and often require multiple NHIs per task, creating complex, opaque identity chains. https://astrix.security/learn/blog/astrix-research-presents-touchpoints-between-ai-and-non-human-identities/

ğŸ“Œ Breaking down â€˜EchoLeakâ€™, Vulnerability Enabling Data Exfiltration from Microsoft 365 Copilotâ€Šâ€”â€Š Itay Ravia and other members of Aim Security identified a vulnerability in Microsoft 365 Copilot where specially crafted emails can trigger data leakage through prompt injection and markdown/CSP bypasses. The issue stems from how Copilot processes untrusted input, potentially exposing internal content. https://www.aim.security/lp/aim-labs-echoleak-blogpost

ğŸ“Œ Remote Prompt Injection in GitLab Duo Leads to Source Code Theftâ€Šâ€”â€Š Legit Securityâ€™s Omer Mayraz demonstrates how a single hidden comment could trigger GitLab Duo (Claude-powered) to leak private source code, suggest malicious packages, and exfiltrate zero-days. The exploit chain combines prompt injection, invisible text, markdown-to-HTML rendering abuse, and access to sensitive contentâ€Šâ€”â€Šshowcasing the real-world risks of deeply integrated AI agents in developer workflows. https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo

ğŸ“Œ ISO/IEC 42005:2025 has been formally published. ISO 42005 provides guidance for organizations conducting AI system impact assessments. Establishing a process and performing an AI system impact assessment is integral for organizations looking to pursue ISO 42001 certification. More importantly, the AI impact assessment allows organizations to identify high risk AI systems and determine any potential impact to individuals, groups, or societies as it relates to fairness, safety, and transparency. https://www.ethos-ai.org/p/ai-impact-checklist or https://www.linkedin.com/posts/noureddine-kanzari-a852a6181_iso-42005-the-standard-of-the-future-activity-7334498579710943233-ts6S

ğŸ“Œ Checklist for LLM Compliance in Governmentâ€Šâ€”â€ŠDeploying AI in government? Compliance isnâ€™t optional. Missteps can lead to fines reaching $38.5M under global regulations like the EU AI Actâ€Šâ€”â€Šor worse, erode public trust. This checklist ensures your government agency avoids pitfalls and meets ethical standards while deploying large language models (LLMs). https://www.newline.co/@zaoyang/checklist-for-llm-compliance-in-government--1bf1bfd0

ğŸ“Œ How I used o3 to find CVE-2025â€“37899, a remote zeroday vulnerability in the Linux kernelâ€™s SMB implementation by Sean Heelan. https://sean.heelan.io/2025/05/22/how-i-used-o3-to-find-cve-2025-37899-a-remote-zeroday-vulnerability-in-the-linux-kernels-smb-implementation/

## ğŸ“„ Reports

ğŸ“˜ Confidential AI Inference Systems â€” Anthropic and Pattern Labs are exploring confidential inferenceâ€Šâ€”â€Šan approach for running AI models on sensitive data without exposing it to infrastructure operators or cloud providers. In a typical AI deployment, three parties are involved: the model owner, the user providing the data, and the cloud provider hosting the service. Without safeguards, each must trust the others with sensitive assets. Confidential inference eliminates this need by enforcing cryptographic boundariesâ€Šâ€”â€Šensuring that neither the data nor the model is accessible outside the secure enclave, not even to the infrastructure host. https://www.linkedin.com/feed/update/urn:li:activity:7341501922383695872

ğŸ“˜ AI Red-Team Playbook for Security Leadersâ€Šâ€”â€Š Hacken, Blockchain Security Auditorâ€™s AI Red-Team Playbook for Security Leaders offers a strategic framework for safeguarding large language model (LLM) systems through lifecycle-based adversarial testing. It identifies emerging risksâ€Šâ€”â€Šprompt injections, jailbreaks, retrieval-augmented generation (RAG) exploits, and data poisoningâ€Šâ€”â€Šwhile emphasizing real-time mitigation and multidisciplinary collaboration. The playbook integrates methodologies like PASTA (Process for Attack Simulation and Threat Analysis) and STRIDE (Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation of Privilege), reinforcing the importance of aligning AI security with enterprise risk governance. https://www.linkedin.com/feed/update/urn:li:activity:7339375352231710721

ğŸ“˜ AI Security Market Report by Latioâ€Šâ€”â€ŠSecurity practitioners have been searching for a resource that clearly describes both what AI security challenges exist, and what solutions the market has provided. As part of this report, Latio surveyed numerous security leaders and found a consistent response: interest in AI Security is high, but itâ€™s still unclear what the actual problems are. This report brings Latioâ€™s characteristic practitioner focused mindset to the problem, highlighting what challenges are out there, and clearly stating the maturity of various vendor offerings to the challenges. https://www.linkedin.com/feed/update/urn:li:activity:7325564352156119040

ğŸ“˜ Fundamentals of Secure AI Systems with Personal Data by Enrico Glerean â€”â€Šis a training for cybersecurity professionals, developers and deployers of AI systems on AI security & Personal Data Protection addressing the current AI needs and skill gaps. https://www.linkedin.com/feed/update/urn:li:activity:7337637796007817216

ğŸ“˜ Security Risks in Artificial Intelligence for Financeâ€Šâ€”â€ŠSet of best practices intended for the Board and C-Level by EFR - European Financial Services Round Table. https://www.linkedin.com/feed/update/urn:li:activity:7341851628955758592

ğŸ“˜ Disrupting malicious uses of AI: June 2025â€Šâ€”â€Š OpenAI continues its work to detect and prevent the misuse of AI, including threats like social engineering, cyber espionage, scams, and covert influence operations. In the last three months, AI tools have helped OpenAIâ€™s teams uncover and disrupt malicious campaigns. Their efforts align with a broader mission to ensure AI is used safely and democraticallyâ€Šâ€”â€Šprotecting people from real harms, not enabling authoritarian abuse. https://www.linkedin.com/feed/update/urn:li:activity:7336790322426912768

ğŸ“˜ Agentic AI Red Teaming Guide by Cloud Security Allianceâ€Šâ€”â€ŠAgentic systems introduce new risksâ€Šâ€”â€Šautonomous reasoning, tool use, and multi-agent complexityâ€Šâ€”â€Šthat traditional red teaming canâ€™t fully address. This guide aims to fill that gap with practical, actionable steps. https://www.linkedin.com/feed/update/urn:li:activity:7333874110684348417

ğŸ“˜ AI Data Securityâ€Šâ€”â€ŠBest Practices for Securing Data Used to Train & Operate AI Systems by Cybersecurity and Infrastructure Security Agencyâ€Šâ€”â€ŠThis guidance highlights the critical role of data security in ensuring the accuracy, integrity, and trustworthiness of AI outcomes. It outlines key risks that may arise from data security and integrity issues across all phases of the AI lifecycle, from development and testing to deployment and operation. https://www.linkedin.com/feed/update/urn:li:activity:7331359099193872387


