# AI Security Digest â€“ July 2025

A digest of AI security research, insights, reports, upcoming events, and tools & resources. Follow the [AI Security community](https://linktr.ee/AISECHUB) on [Twitter](https://twitter.com) and [LinkedIn group](https://linkedin.com) for additional updates.

Sponsored by [InnovGuard.com](https://innovguard.com) â€“ Technology Risk & Cybersecurity Advisory, Innovate and Invest with Confidence, Lead with Assurance.

## ğŸ” Insights

ğŸ“Œ [AIDEFEND: An AI Defense Framework](https://edward-playground.github.io/aidefense-framework/) â€” An open-source knowledge base of defensive countermeasures for AI/ML systems. Maps defenses to known threats from MITRE ATLAS, MAESTRO, and OWASP, offering interactive views for practitioners. Inspired by MITRE D3FEND, ATT&CK, ATLAS, Google SAIF, and OWASP Top 10, but developed independently. By Edward Lee.

ğŸ“Œ [AI slop and fake reports are coming for your bug bounty programs](https://techcrunch.com/2025/07/24/ai-slop-and-fake-reports-are-exhausting-some-security-bug-bounties/) â€” Growing issue of â€œAI slopâ€ bug bounty reports where LLMs fabricate vulnerabilities packaged in professional-looking writeups. Includes cases from CycloneDX and other open-source maintainers. By Lorenzo Franceschi-Bicchierai.

ğŸ“Œ [The Road to Agentic AI: Navigating Architecture, Threats, and Solutions](https://www.trendmicro.com/vinfo/in/security/news/security-technology/the-road-to-agentic-ai-navigating-architecture-threats-and-solutions) â€” Breaks down the multi-layered architecture of agentic AI systems, examining risks at each layer and offering actionable defenses. By Vincenzo Ciancaglini, Marco Balduzzi, Ph.D., Salvatore Gariuolo, Rainer Vosseler, Fernando Tucci.

ğŸ“Œ [Deepfake It Till You Make It: A Comprehensive View of the New AI Criminal Toolset](https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/deepfake-it-til-you-make-it-a-comprehensive-view-of-the-new-ai-criminal-toolset) â€” Survey of deepfake-enabled criminal operations, underground toolkits, and case studies. By David Sancho, Salvatore Gariuolo, Vincenzo Ciancaglini.

ğŸ“Œ [Code Execution Through Deception: Gemini AI CLI Hijack](https://tracebit.com/blog/code-exec-deception-gemini-ai-cli-hijack) â€” Discovery of a Gemini CLI flaw allowing silent malicious command execution via prompt injection and misleading UX. By Sam Cox @ Tracebit.

ğŸ“Œ [An Executive Guide to Secure-by-Design AI](https://mitsloan.mit.edu/ideas-made-to-matter/new-framework-helps-companies-build-secure-ai-systems) â€” Ten strategic questions aligned to AI system development stages for embedding security early in design. By Dr. Keri P. and Nelson Novaes Neto.

ğŸ“Œ [How we Rooted Copilot](https://research.eye.security/how-we-rooted-copilot/) â€” Exploitation of a Microsoft Copilot Enterprise misconfiguration allowing privileged command execution in its Jupyter-based sandbox. By Vaisha Bernard @ Eye Security.

ğŸ“Œ [Artificial Exploits, Real Limitations](https://www.forescout.com/blog/artificial-exploits-real-limitations-how-ai-cyber-attacks-fall-short/) â€” Research showing LLMs are more effective in social engineering, influence ops, and boilerplate malware than in discovering novel vulnerabilities. By Michele Campobasso, Forescout.

ğŸ“Œ [A summer of security: empowering cyber defenders with AI](https://blog.google/technology/safety-security/cybersecurity-updates-summer-2025/) â€” Googleâ€™s Big Sleep agent, leveraging threat intel, discovered and mitigated a live SQLite zero-day (CVE-2025-6965) before exploitation. By Kent Walker.

ğŸ“Œ [11 Questions You Must Ask When Evaluating AI SOC Analysts](https://www.prophetsecurity.ai/blog/11-questions-you-must-ask-when-evaluating-ai-soc-analysts) â€” Key questions for selecting AI SOC vendors, covering capabilities, integration, and governance. By George Dimitrov @ Prophet Security.

ğŸ“Œ [Living Off the Land 2.0: AI-First Platforms, UI Abuse, and Coyote Malware](https://www.akamai.com/blog/security-research/active-exploitation-coyote-malware-first-ui-automation-abuse-in-the-wild) â€” Coyote malware uses native UI automation for stealthy credential theft and surveillance, illustrating UI abuse in AI-first OS environments. By Tomer Peled.

ğŸ“Œ [Detection at Scale â€“ The Cursor Moment for Security Operations](https://www.detectionatscale.com/p/the-cursor-moment-for-security-operations) â€” MCP and AI agents accelerate detection engineering with environment-specific SIEM rules and efficiency gains. By Jack Naglieri.

ğŸ“Œ [Bad Actors are Grooming LLMs to Produce Falsehoods](https://americansunlight.substack.com/p/bad-actors-are-grooming-llms-to-produce) â€” Research into LLM â€œgroomingâ€ to mass-produce disinformation by manipulating model reasoning. By Sophia F., Nina Jankowicz, Gary Marcus.

ğŸ“Œ [AI Vibe Coding Tool Goes Rogue](https://cybernews.com/ai-news/ai-coding-tool-wipes-database-lies/) â€” Case of an AI coding assistant deleting production databases, fabricating users, and hiding test failures.

ğŸ“Œ [Phishing for Gemini](https://0din.ai/blog/phishing-for-gemini) â€” Prompt injection in Google Gemini enabling credential theft via hidden HTML/CSS tags in email summaries. By Marco Figueroa @ 0DIN.ai.

ğŸ“Œ [AI-Generated malicious package found in NPM Registry](https://getsafety.com/blog-posts/threat-actor-uses-ai-to-create-a-better-crypto-wallet-drainer) â€” AI-crafted crypto wallet drainer malware in the NPM registry. By Paul McCarty @ Safety Cybersecurity.

ğŸ“Œ [2025 SANS Institute SOC Survey â€“ Modern SOC Challenges](https://www.sans.org/white-papers/sans-2025-soc-survey) â€” AI/automation expansion planned despite low reported value; warns against replacing SOC analysts with AI. By Seth Misenar.

ğŸ“Œ [Asana MCP server back online after plugging a data-leak hole](https://adversa.ai/blog/asana-ai-incident-comprehensive-lessons-learned-for-enterprise-security-and-ciso/) â€” Logic flaw in beta MCP server exposed cross-tenant data for ~1,000 enterprises. By Alex Polyakov @ Adversa AI.

ğŸ“Œ [GitHub MCP Exploited: Accessing private repositories via MCP](https://invariantlabs.ai/blog/mcp-github-vulnerability) â€” MCP flaw in GitHub integration leaks private repo data via malicious issues. By Marco Milanta & Luca Beurer-Kellner @ Invariant Labs.

ğŸ“Œ [PoC Attack Targeting Atlassianâ€™s MCP](https://www.catonetworks.com/blog/cato-ctrl-poc-attack-targeting-atlassians-mcp/) â€” Prompt injection in Jira Service Management MCP creates â€œLiving Off AIâ€ risks. By Guy Waizel, Dolev Attiya, Shlomo Bamberger @ Cato Networks.

ğŸ“Œ [Agentic Misalignment: How LLMs Could Be Insider Threats](https://www.anthropic.com/research/agentic-misalignment) â€” Anthropic research shows LLM agents in simulated corporate settings choosing harmful actions when goals conflict.

ğŸ“Œ [Code Execution Through Email: How I Used Claude to Hack Itself](https://www.pynt.io/blog/llm-security-blogs/code-execution-through-email-how-i-used-claude-mcp-to-hack-itself) â€” MCP-enabled Claude exploit chain from crafted Gmail message to code execution. By Golan Yosef @ Pynt.

ğŸ“Œ [10 Ways AI is Enhancing Ransomware-as-a-Service](https://www.linkedin.com/feed/update/urn:li:activity:7353604333646606336) â€” AI-powered RaaS tactics: ransom chatbots, recon, adaptive encryption, dynamic pricing, deepfake voice extortion.

ğŸ“Œ [ISC2 Survey â€“ AI in Cybersecurity](https://www.isc2.org/Insights/2025/07/2025-isc2-ai-pulse-survey) â€” 42% testing AI security tools, 70% report improved effectiveness; notes new AI-focused roles emerging.

ğŸ“Œ [The Race to Secure Enterprise AI â€“ Insight Partners](https://www.insightpartners.com/ideas/securing-ai/) â€” Market view on secure model dev, runtime AI firewalls, and threat detection guardrails.

ğŸ“Œ [Malware with Embedded Prompt Injection](https://research.checkpoint.com/2025/ai-evasion-prompt-injection) â€” Skynet malware embeds prompt injection to evade AI analysis tools.

ğŸ“Œ Security for Agents and Agents for Security â€” [Menlo Ventures blog](https://menlovc.com/) on why securing AI agents requires rethinking authentication, privileges, and observability.

ğŸ“Œ [Repeater Strike: Manual Testing, Amplified](https://portswigger.net/research/repeater-strike-manual-testing-amplified) â€” Burp Suite extension using AI to generate regex rules for IDOR detection. By Gareth Heyes.

ğŸ“Œ [AI Cybersecurity Careers: The Complete Guide](https://robtlee73.substack.com/p/ai-cybersecurity-careers-the-complete) â€” Breakdown of emerging AI-security hybrid roles. By Rob T. Lee @ SANS.

ğŸ“Œ [Checklist for LLM Compliance in Government](https://www.newline.co/@zaoyang/checklist-for-llm-compliance-in-government--1bf1bfd0) â€” Compliance steps for LLM deployment in government agencies. By Sizhao Yang.

ğŸ“Œ [Grok-4 Jailbreak with Echo Chamber and Crescendo](https://neuraltrust.ai/blog/grok-4-jailbreak-echo-chamber-and-crescendo) â€” Multi-turn jailbreak achieving up to 67% success on harmful prompts. By Ahmed Alobaid @ NeuralTrust.

ğŸ“Œ [NVIDIAScape â€“ Critical NVIDIA AI Vulnerability](https://www.wiz.io/blog/nvidia-ai-vulnerability-cve-2025-23266-nvidiascape) â€” CVE-2025-23266 container escape in NVIDIA Toolkit enabling root access. By Nir Ohfeld & Shir Tamari @ Wiz.

ğŸ“Œ [Hacker Plants Computer 'Wiping' Commands in Amazon's AI Coding Agent](https://www.404media.co/hacker-plants-computer-wiping-commands-in-amazons-ai-coding-agent) â€” Malicious PR in Amazon Q added destructive wipe commands. By Joseph Cox.

## ğŸ“„ Reports

ğŸ“˜ [SBOM for AI Use Cases](https://www.linkedin.com/feed/update/urn:li:activity:7352420212471709697) â€” Community-driven resource for applying SBOM practices to AI systems. Covers business, legal, and security risks from GenAI and LLMs, mirroring software supply chain challenges. Provides a standardized approach to improve transparency, trust, and governance. Authors: Helen Oakley, Daniel Bardenstein, Dmitry R.

ğŸ“˜ [Securing Agentic Applications Guide](https://www.linkedin.com/feed/update/urn:li:activity:7355648811236511745) â€” Practical, actionable guidance for designing and deploying secure agentic applications powered by LLMs. Complements OWASP Agentic AI Threats & Mitigations with concrete technical recommendations for builders and defenders.

ğŸ“˜ [Americaâ€™s AI Action Plan â€“ 12 AI Cybersecurity Priorities](https://www.linkedin.com/feed/update/urn:li:activity:7353987917704294400) â€” U.S. strategy for securing AI, including secure-by-design dev, AI incident response, AI-ISAC threat intel sharing, deepfake detection standards, and model risk evaluations. Targets IP protection, critical infrastructure defense, red-teaming, and export controls.

ğŸ“˜ [Google's Approach for Secure AI Agents](https://www.linkedin.com/feed/update/urn:li:activity:7347701762813829120) â€” Framework for secure AI agents combining deterministic controls with dynamic, reasoning-based defenses. Principles: clear human controllers, limited powers, and full observability. By Santiago DÃ­az MuÃ±oz, Christoph Kern, Kara Olive.

ğŸ“˜ [Preparing Defenders of AI Systems V1.0](https://github.com/cosai-oasis/ws2-defenders/blob/main/preparing-defenders-of-ai-systems.md) â€” Coalition for Secure AI paper on shifting enterprise security priorities from models to agents. Highlights layered defenses, governance gaps, and AI-specific strategies.

ğŸ“˜ [AI Controls Matrix (AICM)](https://cloudsecurityalliance.org/artifacts/ai-controls-matrix) â€” CSA vendor-agnostic framework with 243 controls across 18 domains for secure, responsible AI. Maps to ISO 42001, ISO 27001, NIST AI RMF 1.0, and BSI AIC4.

ğŸ“˜ [AI Safety Practices Compared â€“ 2025 FLI Report](https://www.linkedin.com/feed/update/urn:li:activity:7352842152759971840) â€” Evaluation of Anthropic, OpenAI, DeepMind, Meta, xAI, Zhipu AI, and DeepSeek across 33 safety indicators. Finds gaps in cyber misuse testing, red-teaming, incident reporting, and bug bounties.

ğŸ“˜ [AI Risk Trends â€“ 2025 Team8 CISO Village](https://www.linkedin.com/feed/update/urn:li:activity:7353987917704294400) â€” Survey of 110+ CISOs: 67% use AI agents, 25% faced AI-driven attacks, 77% expect AI to replace SOC tasks. Shadow AI governance remains weak.

ğŸ“˜ [Understanding and Safeguarding Childrenâ€™s Use of AI Chatbots](https://www.linkedin.com/feed/update/urn:li:activity:7351249793546924045) â€” Risks include misinformation, harmful content, emotional dependence, and privacy issues. Calls for age-appropriate design and stronger moderation.

ğŸ“˜ [AI Coding Assistants: Security-Safe Navigation](https://www.linkedin.com/feed/update/urn:li:activity:7351007465494171649) â€” Secure Code Warrior report: top LLMs only ~47% correct and secure; insecure coding patterns persist. Highlights misconfigurations, lack of runtime awareness, and supply chain risks.

ğŸ“˜ [Cyber and Artificial Intelligence Risk in Financial Services](https://www.linkedin.com/feed/update/urn:li:activity:7347449832065613825) â€” DFSA report on AI and cyber risks in financial services. By Justin Baldacchino and Herman Schueller.

ğŸ“˜ [The AI Tech Stack: A Primer for Tech and Cyber Policy](https://www.linkedin.com/feed/update/urn:li:activity:7348456040172048385) â€” Paladin Capital Group defines five AI stack layers and stresses integrating security across governance, application, infrastructure, models, and data.

ğŸ“˜ [AI Maturity Model for Cybersecurity](https://www.linkedin.com/feed/update/urn:li:activity:7352156064244514816) â€” Darktraceâ€™s 5-level model from Manual Operations to AI Delegation, guiding CISOs toward autonomous defense with governance.

ğŸ“˜ [The SAIL (Secure AI Lifecycle) Framework](https://www.linkedin.com/feed/update/urn:li:activity:7346638798056800257/) â€” Pillar Securityâ€™s practical guide for building and deploying secure AI applications, authored by 20+ industry experts.

ğŸ“˜ [State of Cybersecurity Resilience 2025](https://www.linkedin.com/feed/update/urn:li:activity:7350979113358184451) â€” Accenture report: 90% of orgs lack maturity for modern AI threats, 77% miss foundational AI security practices. Recommends governance, AI-secure cores, and resilient systems.

ğŸ“˜ [Databricks AI Governance Framework](https://www.linkedin.com/feed/update/urn:li:activity:7346374987928260613) â€” Guide for responsible, effective enterprise AI programs. By David Wells and Abhi A.

ğŸ“˜ [State of LLM Application Security](https://www.linkedin.com/feed/update/urn:li:activity:7352038190066651136) â€” Cobalt report: 32% of LLM pentest issues are high/critical. Top risks include prompt injection, data leaks, poisoning, and bias; only 21% of serious AI vulns remediated.

ğŸ“˜ [Multi-Layered AI Defense](https://www.linkedin.com/feed/update/urn:li:activity:7352381103879475203) â€” Darktrace outlines unsupervised, supervised, and generative AI in a continuous Learn â†’ Detect â†’ Investigate â†’ Respond â†’ Re-learn cycle with human oversight.

ğŸ“˜ [Trustworthiness for AI in Defence](https://eda.europa.eu/docs/default-source/brochures/taid-white-paper-final-09052025.pdf) â€” European Defence Agency white paper on trusted AI, verification/validation, and certification requirements.

ğŸ“˜ [The Mitigating â€˜Hiddenâ€™ AI Risks Toolkit](https://www.linkedin.com/feed/update/urn:li:activity:7349585719247446016) â€” UK Government Communications guide for managing unintended AI risks. Draws from lessons deploying the â€œAssistâ€ GenAI tool.

ğŸ“˜ [SAFE-AI: A Framework for Securing AI-Enabled Systems](https://www.linkedin.com/feed/update/urn:li:activity:7347734136868020224) â€” MITRE framework addressing supply chain, adversarial inputs, poisoning, bias, and sensitive data exposure. By John Kressel.

ğŸ“˜ [The General-Purpose AI Code of Practice â€“ Safety & Security](https://www.linkedin.com/feed/update/urn:li:activity:7349138123362091009) â€” EU voluntary framework under the AI Act for managing systemic risks in advanced models, developed via multi-stakeholder process.

## ğŸ“‚ Upcoming Events

ğŸ“… [The AI Summit at Black Hat](https://www.blackhat.com/us-25/ai-summit.html) â€” August 5, 2025 | Mandalay Bay, Las Vegas, USA  

ğŸ“… [AI Village @ DEF CON 33](https://aivillage.org/events/defcon33/) â€” August 7, 2025 | Las Vegas Convention Center, Las Vegas, USA  

ğŸ“… [Vegas AI Security Forum â€˜25](https://aisecurity.forum/vegas-25) â€” August 7, 2025 | 10:00 AM â€“ 11:00 PM | Palms Casino Resort, Las Vegas, USA  

ğŸ“… [GRC Data & AI Summit](https://www.anecdotes.ai/grc-data-ai-summit) â€” August 13, 2025 | 9:00 AM PDT | Virtual | By Anecdotes  

ğŸ“… [Artificial Intelligence Risk Summit](https://www.airisksummit.com/) â€” August 19â€“20, 2025 | Virtual & In-person  

ğŸ“… [Agentic AI Security Summit 2025](https://web.cvent.com/event/6a48aa4b-a67c-40ca-81a5-3c3e8f78884c/) â€” August 19, 2025 | Cloud Security Alliance | Virtual  

ğŸ“… [The International Conference on Cybersecurity and AI-Based Systems](https://www.cyber-ai.org/) â€” September 1â€“4, 2025 | Bulgaria  

ğŸ“… [HackATHon 2025](https://hackaicon.ethiack.com/) â€” September 25, 2025 | LX Factory, Lisbon, Portugal | By ETHIACK  

ğŸ“… [The AI Summit at Security Education Conference Toronto (SecTor) 2025](https://www.blackhat.com/sector/2025/ai-summit.html) â€” September 30, 2025 | MTCC, Toronto, Ontario, Canada


