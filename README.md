# AI Security Digest ‚Äì August 2025

https://pistachioapp.com/blog/copilot-broke-your-audit-logA digest of AI security research, insights, reports, upcoming events, and tools & resources. Follow AI Security community on Twitter and LinkedIn group for additional updates.Also check out our new project, Awesome AI Security.

Sponsored by InnovGuard.com - Technology Risk & Cybersecurity Advisory - Innovate and Invest with Confidence, Lead with Assurance.

# üîç Insights
üìå Johann Rehberger spent the past month publishing a daily ‚ÄúMonth of AI Bugs‚Äù series documenting prompt-injection flaws in coding agents and developer tools that lead to data leaks, RCEs in GitHub Copilot, Claude Code, Amazon Q Developer, Windsurf, OpenHands, Devin, and Cursor. https://embracethered.com/blog/ https://youtube.com/@embracethered

üìå MCP Security is a Cloud Security Alliance community project focused on providing security guidance, best practices, and tools for safely deploying MCP servers and AI agents.

- MCP Client Top 10- https://modelcontextprotocol-security.io/top10/client/
- MCP Server Top 10 - https://modelcontextprotocol-security.io/top10/server/
- MCP Security Tactics, Techniques, and Procedures (TTPs) - https://modelcontextprotocol-security.io/ttps/ 

üìå [Unleashing the Hound: How AI Agents Find Deep Logic Bugs in Any Codebase](https://muellerberndt.medium.com/unleashing-the-hound-how-ai-agents-find-deep-logic-bugs-in-any-codebase-64c2110e3a6f) - Hound is a language-agnostic AI code security auditor that simulates the cognitive processes of human experts. It maps systems as living knowledge graphs, and uses focused, high-quality hypotheses from strong reasoning models to find deep logic bugs across any stack. In this post, I explain how Hound works, and show how to run it on the codebase of an actual audit contest. By Bernhard Mueller @ Spearbit Labs - https://muellerberndt.medium.com/unleashing-the-hound-how-ai-agents-find-deep-logic-bugs-in-any-codebase-64c2110e3a6f

üìå [Copilot Broke Your Audit Log, but Microsoft Won‚Äôt Tell You](https://pistachioapp.com/blog/copilot-broke-your-audit-log) - On July 4th, I came across a problem in M365 Copilot: Sometimes it would access a file and return the information, but the audit log would not reflect that. Upon testing further, I discovered that I could simply ask Copilot to behave in that manner, and it would. That made it possible to access a file without leaving a trace. Given the problems that creates, both for security and legal compliance, I immediately reported it to Microsoft through their MSRC portal. By Zack Korman at Pistachio https://pistachioapp.com/blog/copilot-broke-your-audit-log

üìå [First Known AI-Powered Ransomware Uncovered (PoC)](https://welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/) - The discovery of PromptLock shows how malicious use of AI models could supercharge ransomware and other threats. ESET researchers have discovered what they called "the first known AI-powered ransomware". The malware, which ESET has named PromptLock, has the ability to exfiltrate, encrypt and possibly even destroy data, though this last functionality appears not to have been implemented in the malware yet. While PromptLock was not spotted in actual attacks and is instead thought to be a proof-of-concept (PoC) or a work in progress, ESET's discovery shows how malicious use of publicly-available AI tools could supercharge ransomware and other pervasive cyberthreats. By Anton Cherepanov and Peter Str√Ωƒçek  ESET - https://welivesecurity.com/en/ransomware/first-known-ai-powered-ransomware-uncovered-eset-research/

üìå [All You Need Is MCP - LLMs Solving a DEF CON CTF Finals Challenge](https://wilgibbs.com/blog/defcon-finals-mcp/)  - After spending two years with LLMs for AIxCC, I‚Äôve come to have a feeling for the boundary of what LLMs are and are not capable of. This is the FIRST time I‚Äôve seen a challenge at the difficulty level of DEF CON Finals CTF solved purely with LLMs (extremely little human interaction). We had some excellent hackers from our team working on the challenge "ico" for about four hours prior to my starting (salls, x3ero0, zardus, etc.), so it‚Äôs not like this was exactly an easy challenge. I thought it was important for everyone else in the community to see it happen too! By Wil Gibbs https://wilgibbs.com/blog/defcon-finals-mcp/ 

üìå [Weaponizing image scaling against production AI systems](https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/) - Picture this: you send a seemingly harmless image to an LLM and suddenly it exfiltrates all of your user data. By delivering a multi-modal prompt injection not visible to the user, we achieved data exfiltration on systems including the Google Gemini CLI. This attack works because AI systems often scale down large images before sending them to the model: when scaled, these images can reveal prompt injections that are not visible at full resolution. By Kikimora Morozova and Suha S. Hussain at Trail of Bits - https://blog.trailofbits.com/2025/08/21/weaponizing-image-scaling-against-production-ai-systems/

üìå [Critical Vulnerability in AI Vibe Coding platform Base44 Allowing Unauthorized Access to Private Applications](https://www.wiz.io/blog/critical-vulnerability-base44) - The vulnerability discovered was remarkably simple to exploit - by providing only a non-secret app_id value to undocumented registration and email verification endpoints, an attacker could have created a verified account for private applications on their platform. By Gal Nagli at Wiz - https://www.wiz.io/blog/critical-vulnerability-base44

üìå [How We Exploited CodeRabbit: From a Simple PR to RCE and Write Access on 1M Repositories](https://research.kudelskisecurity.com/2025/08/19/how-we-exploited-coderabbit-from-a-simple-pr-to-rce-and-write-access-on-1m-repositories/) - By Nils Amiet at Kudelski Security -  https://research.kudelskisecurity.com/2025/08/19/how-we-exploited-coderabbit-from-a-simple-pr-to-rce-and-write-access-on-1m-repositories/

üìå [Breaking NVIDIA Triton](https://www.wiz.io/blog/nvidia-triton-cve-2025-23319-vuln-chain-to-ai-server) - The Wiz Research team has discovered a chain of critical vulnerabilities in NVIDIA's Triton Inference Server, a popular open-source platform for running AI models at scale. When chained together, these flaws can potentially allow a remote, unauthenticated attacker to gain complete control of the server, achieving remote code execution (RCE). By Ronen Shustin and Nir Ohfeld - https://www.wiz.io/blog/nvidia-triton-cve-2025-23319-vuln-chain-to-ai-server

üìå [GPT-5 AI Router Novel Vulnerability Class Exposes the Fatal Flaw in Multi-Model Architectures](https://adversa.ai/blog/promisqroute-gpt-5-ai-router-novel-vulnerability-class/) - When you use ChatGPT or any major AI service, you think you‚Äôre talking to one AI model. You‚Äôre not. Behind the scenes, a ‚Äúrouter‚Äù reads your message and decides which of many models should answer‚Äîusually picking the cheapest one, not the safest. Meet PROMISQROUTE ‚Äî a fundamentally new AI vulnerability that abuses AI routing mechanism to trigger SSRF-style bypass in multimodal infrastructure leading to ChatGPT Model Downgrade and Jailbreak exploitation as an example. By Adversa AI - https://adversa.ai/blog/promisqroute-gpt-5-ai-router-novel-vulnerability-class/

üìå [InversePrompt: Turning Claude Against Itself, One Prompt at a Time](https://cymulate.com/blog/cve-2025-547954-54795-claude-inverseprompt/) - As Anthropic‚Äôs Claude Code gains traction as a powerful AI coding assistant, it promises developers a safe and streamlined way to build with Claude‚Äôs capabilities. But what happens when the same assistant meant to enforce restrictions unknowingly reveals how to bypass them? By Elad Beber at Cymulate - https://cymulate.com/blog/cve-2025-547954-54795-claude-inverseprompt/ 

üìå [Every Reason Why I Hate AI and You Should Too](https://malwaretech.com/2025/08/every-reason-why-i-hate-ai.html) - One thing that‚Äôs certain is that Generative AI is in a bubble. That‚Äôs not to say AI as a technology will pop, or that there isn‚Äôt genuine room for a lot more growth; simply, the level of hype far outweighs the current value of the tech. By Marcus Hutchins - https://malwaretech.com/2025/08/every-reason-why-i-hate-ai.html

üìå [Perplexity is using stealth, undeclared crawlers to evade website no-crawl directives](https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/) - We are observing stealth crawling behavior from Perplexity, an AI-powered answer engine. Although Perplexity initially crawls from their declared user agent, when they are presented with a network block, they appear to obscure their crawling identity in an attempt to circumvent the website‚Äôs preferences. We see continued evidence that Perplexity is repeatedly modifying their user agent and changing their source ASNs to hide their crawling activity, as well as ignoring ‚Äî or sometimes failing to even fetch ‚Äî robots.txt files. By Gabriel A., Vaibhav Singhal, Brian Mitchell, Reid Tatoris - https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/

üìå [Safeguarding VS Code against prompt injections](https://github.blog/security/vulnerability-research/safeguarding-vs-code-against-prompt-injections/) - Michael Stepankin shares several exploits I discovered during my security assessment of the Copilot Chat extension, specifically regarding agent mode, and that we‚Äôve addressed together with the VS Code team. These vulnerabilities could have allowed attackers to leak local GitHub tokens, access sensitive files, or even execute arbitrary code without any user confirmation. I‚Äôll also discuss some unique features in VS Code that help mitigate these risks and keep you safe. Finally, I‚Äôll explore a few additional patterns you can use to further increase security around reading and editing code with VS Code. https://github.blog/security/vulnerability-research/safeguarding-vs-code-against-prompt-injections/ 

üìå [Prompt Mines: 0-Click Data Corruption In Salesforce Einstein](https://labs.zenity.io/p/prompt-mines-0-click-data-corruption-in-salesforce-einstein-1cfb) - Salesforce Einstein is Salesforce‚Äôs flagship AI assistant, integrated into the Salesforce CRM. In a previous post we took a deep dive into Einstein‚Äôs architecture, mentioning how Salesforce built Einstein so it doesn‚Äôt read tool call results but instead immediately renders them onto the screen, making indirect prompt injections almost impossible (since no LLM reasons on the tool‚Äôs response). By Tamir Ishay Sharbat at Zanity - https://labs.zenity.io/p/prompt-mines-0-click-data-corruption-in-salesforce-einstein-1cfb

üìå [GPT-5 Jailbreak with Echo Chamber and Storytelling](https://neuraltrust.ai/blog/gpt-5-jailbreak-with-echo-chamber-and-storytelling) - We document how we achieved a jailbreak of gpt-5-chat using the Echo Chamber algorithm paired with narrative-driven steering (storytelling). By Mart√≠ Jord√† Roca at NeuralTrust - https://neuraltrust.ai/blog/gpt-5-jailbreak-with-echo-chamber-and-storytelling 

üìå [Prompt injection engineering for attackers: Exploiting GitHub Copilot](https://blog.trailofbits.com/2025/08/06/prompt-injection-engineering-for-attackers-exploiting-github-copilot/) - The exploit allows an attacker to file an issue for an open-source software project that tricks GitHub Copilot (if assigned to the issue by the project‚Äôs maintainers) into inserting a malicious backdoor into the software. While this blog post is just a demonstration, we expect the impact of attacks of this nature to grow in severity as the adoption of AI agents increases throughout the industry. By Kevin Higgs - https://blog.trailofbits.com/2025/08/06/prompt-injection-engineering-for-attackers-exploiting-github-copilot/ 

üìå [Lessons learned building an AI hacker | AI Cyber Challenge](https://theori.io/blog/building-effective-llm-agents-63446) - The AI Cyber Challenge (AIxCC) challenged competitors to build an autonomous Cyber Reasoning System (CRS) capable of finding, triggering, and patching security vulnerabilities in large codebases. With sufficient cloud compute and access to leading LLM providers, there are countless ways for teams to approach this challenge. Perhaps the most natural approach is to use traditional automated security testing techniques (such as fuzzing) to find crashes, and then use the proven coding abilities of LLMs to generate patches fixing the identified crashes. This ‚Äúfuzzing-first‚Äù approach makes limited use of LLM capabilities, but can still provide a foundation for a competitive CRS submission. By Theori https://theori.io/blog/building-effective-llm-agents-63446

üìå [Atlantis Infrastructure](https://team-atlanta.github.io/blog/post-atl-infra/) - Team Atlanta share how they designed the infrastructure of CRS, Atlantis, to meet these keys and make it as robust as possible. https://team-atlanta.github.io/blog/post-atl-infra/ 

üìå [MCP vulnerability case study: SQL injection in the Postgres MCP server](https://securitylabs.datadoghq.com/articles/mcp-vulnerability-case-study-SQL-injection-in-the-postgresql-mcp-server/) - We found a SQL injection vulnerability in Anthropic‚Äôs reference Postgres MCP server that allowed us to bypass the read-only restriction and execute arbitrary SQL statements. By Santiago M. Mola at Datadog https://securitylabs.datadoghq.com/articles/mcp-vulnerability-case-study-SQL-injection-in-the-postgresql-mcp-server/

üìå [Ollama Drive-By Attack POC](https://gitlab-com.gitlab.io/gl-security/security-tech-notes/red-team-tech-notes/ollama-driveby/) - The PoC was confirmed to work against both Windows and MacOS victim machines (the systems browsing to the malicious website). On MacOS, the JavaScript port-scanning runs quickly. On Windows, the same code runs much more more slowly. Chris Moberly and Charlie Ablett at GitLab https://gitlab-com.gitlab.io/gl-security/security-tech-notes/red-team-tech-notes/ollama-driveby/ 

üìå [Introducing Amazon Bedrock AgentCore Identity: Securing agentic AI at scale](https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-identity-securing-agentic-ai-at-scale/) - As organizations deploy AI agents into production environments, they face a critical challenge: how to securely manage identity and access at scale. Applications need to authenticate users for invoking AI agents, and these agents need to access multiple tools and services, maintain audit trails, and integrate with existing enterprise identity systems‚Äîall while avoiding data leakage and maintaining compliance with organizational requirements. These requirements become exponentially more complex when agents operate across disparate systems, act on behalf of different users, and need to access resources and tools in both AWS and external third-party services. By Rahul Sharma (Aus PR), Fei Yuan, Satveer Khurpa, and Antonio Rodriguez. https://aws.amazon.com/blogs/machine-learning/introducing-amazon-bedrock-agentcore-identity-securing-agentic-ai-at-scale/

üìå [The Silent Exfiltration: Zero-Click Agentic AI Hack That Can Leak Your Google Drive with One Email](https://www.straiker.ai/blog/the-silent-exfiltration-zero-click-agentic-ai-hack-that-can-leak-your-google-drive-with-one-email) - Zero-click exploits can hijack AI agents to exfiltrate Google Drive data, no user interaction needed. See how attack chains form, why autonomy is dangerous, and how runtime guardrails catch what others miss. By Amanda R., Dan R., Vinay K. at Straiker - https://www.straiker.ai/blog/the-silent-exfiltration-zero-click-agentic-ai-hack-that-can-leak-your-google-drive-with-one-email 

üìå [Project Ire autonomously identifies malware at scale](https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/) - autonomous AI agent that can analyze and classify software without assistance, a step forward in cybersecurity and malware detection. The prototype, Project Ire, automates what is considered the gold standard in malware classification: fully reverse engineering a software file without any clues about its origin or purpose. It uses decompilers and other tools, reviews their output, and determines whether the software is malicious or benign. By Brian Caswell, Dustin Fraze, Sarah Armstrong-Smith, Rodrigo Martins Racanicci, Tim M., Shelby H., Stanley He, Katy Smith, Bhakta Pradhan, Mike Walker - https://www.microsoft.com/en-us/research/blog/project-ire-autonomously-identifies-malware-at-scale/

üìå [Unpacking the Microsoft 365 Copilot Attack Surface](https://guardz.com/blog/unpacking-the-microsoft-365-copilot-attack-surface/) - The Microsoft 365 Copilot Attack Matrix is a structured way to think like an adversary in this AI-assisted environment. It maps real-world offensive tactics to the AI-enhanced capabilities of Copilot, showing how Reconnaissance, Initial Access, Discovery, Persistence, Lateral Movement, Exfiltration, and C2 can be reimagined in an AI-driven productivity suite. By Elli Shlomo (IR) at Guardz. https://guardz.com/blog/unpacking-the-microsoft-365-copilot-attack-surface/

üìå AI Agent Security Market Leaders 2025 - Jason Saltzman from CB Insights, published the chart of market leaders. This growth is part of a broader AI security ecosystem valued projected to exceed $134B by 2030.  

üìå Drive-By Attack in Ollama Desktop - Ollama is a popular local AI application that lets you run LLMs on your own machine. Many users choose it to keep their AI conversations private, without sending data to third parties. As of this writing, it has 150,000 stars on GitHub - making it one of the top-ranked AI tools. There was a vulnerability in the macOS/Windows desktop GUI (not the core API) which would have allowed any website you visit to reconfigure your local application settings - sending all of your local chats to a remote, attacker-controlled server. This means that every private conversation could have been intercepted and read remotely, and every response potentially modified using poisoned models. By Chris Moberly and Charlie Ablett at GitLab. 


# üìÑ Reports

üìò [MLOps Overview](https://www.linkedin.com/feed/update/urn:li:activity:7366954293167325186) - This comprehensive overview explores how DevSecOps practices apply to the ML lifecycle through MLOps, along with Large Language Model Operations (LLMOps), and AI Agent Operations (AgentOps). It reveals that traditional security approaches are insufficient for ML systems due to novel threats such as data poisoning, model inversion, adversarial attacks, and member inference attacks. Led Roupe Sahans, Abdul Rahman Sattar, Julianna Tchebotareva, Karan Goenka by Cloud Security Alliance https://www.linkedin.com/feed/update/urn:li:activity:7366954293167325186 

üìò [Detecting and countering misuse of AI](https://www.linkedin.com/feed/update/urn:li:activity:7366579281214308353) - This represents the work of Threat Intelligence: a dedicated team at Anthropic finds deeply investigated sophisticated real world cases of misuse and works with the rest of the Safeguards organization to improve our defenses against such cases. Led Alex Moix, Ken L., Jake Klein at Anthropic https://www.linkedin.com/feed/update/urn:li:activity:7366579281214308353 

üìò [Agentic AI Identity and Access Management: A New Approach](https://www.linkedin.com/feed/update/urn:li:activity:7363712903067127808) - This publication from the Cloud Security Alliance (CSA) introduces a purpose-built Agentic AI IAM framework that accounts for autonomy, ephemerality, and delegation patterns of AI agents in complex Multi-Agent Systems (MAS). It provides security architects and identity professionals with a blueprint to manage agent identities using Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and Zero Trust principles, while addressing operational challenges like secure delegation, policy enforcement, and real-time monitoring. By Ken Huang, Vineeth Sai Narajala, John Yeoh, Jason Ross, Mahesh Lambe, Ramesh Raskar, youssef_ H, Jerry Huang, Idan Habler, PhD, Chris Hughes, Akram Sheriff, Staford Titus S https://www.linkedin.com/feed/update/urn:li:activity:7363712903067127808 

üìò [OWASP AI Maturity Assessment + Excel Assessment Tool](https://www.linkedin.com/feed/update/urn:li:activity:7361898638551240705) - AIMA supports organizations in evaluating how well their AI systems align with strategic goals, ethical principles, and operational needs. The model spans five core domains: Strategy, Design, Implementation, Operations, and Governance. Each domain includes actionable maturity levels to guide adoption and improvement. By Matteo Meucci, Philippe Schrettenbrunner, Arvinda G., Sana Zia Hassan, Abhinavdutt Singh, Marco Denti, Andrea Luigi Vitali, Hubert Jackowski, Keren Katz, Montadhar REKAYA https://www.linkedin.com/feed/update/urn:li:activity:7361898638551240705 

üìò [Design Principles for LLM-based Systems with Zero Trust](https://www.linkedin.com/feed/update/urn:li:activity:7361109040367611904) - Six principles from Federal Office for Information Security (BSI) and ANSSI - Agence nationale de la s√©curit√© des syst√®mes d'information outline how to apply Zero Trust to LLM-based systems, reducing attack surfaces, detecting threats early, and ensuring safe, transparent operation. https://www.linkedin.com/feed/update/urn:li:activity:7361109040367611904 

üìò [Strengthening Emergency Preparedness and Response for AI Loss of Control Incidents](https://www.linkedin.com/feed/update/urn:li:activity:7361039631129227266) - By Elika ≈û., Anjay Friedman, Henry Wu, Marianne Lu, Christopher Byrd, Henry Van Soest, Sana Zakaria https://www.linkedin.com/feed/update/urn:li:activity:7361039631129227266 

üìò [AI Security Solutions Landscape - Agentic AI Q3 2025](https://www.linkedin.com/feed/update/urn:li:activity:7360665992500273152) - The Solutions Landscape monitors and maps the full Agentic AI lifecycle, focusing on the DevOps‚ÄìSecOps intersection to meet evolving security needs. Guided by the Agentic AI Threats and Mitigations guide and SecOps tasks, it highlights open-source and commercial solutions by stage, identifying their coverage of Agentic SecOps duties and threat mitigation, and leverages industry and community input as a peer-reviewed resource for navigating agentic AI‚Äôs shifting security challenges. Updated Quarterly. By Scott Clinton, Andy Smith, Arun John, Aurora Starita, Blanca Rivera, Bryan Nakayama, Dan Guido, Dennys Kaluzny Pereira, Emmanuel Guilherme, Fabrizio Cilli, Garvin LeClaire, Heather Linn, Helen Oakley, Ishan Anand, Jason Ross, Joshua O., Marcel Winandy, Markus Hupfauer, Migel Fernandez, Mohit Yadav, Rachel James, Rammohan Thirupasur, Rico Komenda, Talesh Seeparsan, Todd Hathaway, Ron F. Del Rosario, Vaibhav Malik, Teruhiro Tagomori https://www.linkedin.com/feed/update/urn:li:activity:7360665992500273152 

üìò [State of Agentic AI Security and Governance](https://www.linkedin.com/feed/update/urn:li:activity:7359398882205683713) - The State of Agentic AI Security and Governance provides a comprehensive view of today‚Äôs landscape for securing and governing autonomous AI systems. It explores the frameworks, governance models, and global regulatory standards shaping responsible Agentic AI adoption. Designed for developers, security professionals, and decision-makers, the report serves as a practical guide for navigating the complexities of building, managing, and deploying agentic applications safely and effectively. By Kayla Underkoffler, Rock Lambros, Evgeniy Kokuykin, Keren Katz, Joshua Beck, Allie Howe, Ken Huang, Sumit Ranjan, Vineeth Sai Narajala, Josh Devon, Victor Lu, Abhineeth Pasam, Kellen Carl, Ron Herradian, Ninad Doshi, Nayan Goel, Brian Boyd, John Sotiropoulos, Ron F. Del Rosario https://www.linkedin.com/feed/update/urn:li:activity:7359398882205683713 

üìò [A Practical Guide for Building Robust AI/ML Pipeline Security](https://www.linkedin.com/feed/update/urn:li:activity:7358835243413360641) - An overview of DevSecOps practices that are applicable to MLSecOps. Lessons learned from DevSecOps can proactively address security challenges in the emerging AI/ML lifecycle. By Andrey Shorov, Elif √úst√ºndaƒü Soykan, Ph.D., CISSP, Sarah Evans, Bahaulddin Shammary, Attila Ulbert, PhD, G√∏ran Hall, Jim Reno, ‚öôÔ∏è Eddie Knight, Chris Robinson, David A. Wheeler, Mihai (MM) Maruseac, Elif √úst√ºndaƒü Soykan, Ph.D., CISSP, Rob Moffat https://www.linkedin.com/feed/update/urn:li:activity:7358835243413360641 

üìò [Smart Cities and Critical Infrastructure AI Security Framework](https://www.linkedin.com/feed/update/urn:li:activity:7358680697135403010) - The Smart Cities Critical Infrastructure (SCCI) AI Framework is designed to provide a comprehensive, sector-agnostic approach to securing and optimizing the use of artificial intelligence across urban critical infrastructure domains. As cities increasingly rely on interconnected digital systems‚Äîranging from emergency dispatch and healthcare to law enforcement and utilities‚Äîthe need for robust, adaptable, and transparent AI governance becomes paramount. This framework addresses the unique challenges posed by AI integration, including data privacy, operational resilience, regulatory compliance, and the mitigation of emerging cyber threats. https://www.linkedin.com/feed/update/urn:li:activity:7358680697135403010 

üìò [Secure Agentic System Design: A Trait-Based Approach](https://www.linkedin.com/feed/update/urn:li:activity:7357415085222318080) - CSAThis publication from the CSA AI Technology and Risk Working Group addresses the unique security challenges of agentic AI. As AI transitions from passive tools to autonomous decision-makers, traditional security frameworks struggle to contextualize these new risks. Instead, we need a trait-based approach to agentic system security that identifies fundamental patterns in agent behavior and their associated vulnerabilities. By Nate Lee, Ken Huang, Akram Sheriff, Manish Mishra, Aditya G., Victor L., Michael Roza , Scotty Andrade https://www.linkedin.com/feed/update/urn:li:activity:7357415085222318080 

üìò [The AI Oversight Gap - Cost of a Data Breach Report 2025](https://www.linkedin.com/feed/update/urn:li:activity:7357597342088224769) - The report combines data from 600 actual breaches with 3,470 interviews of security and business leaders. It uses activity-based costing to quantify breach impacts across detection, response, notification, and lost business. https://www.linkedin.com/feed/update/urn:li:activity:7357597342088224769 

# üìÖ Upcoming Events

üìÖ The International Conference on Cybersecurity and AI-Based Systems ‚Äî September 1‚Äì4, 2025 | Varna, Bulgaria | https://www.cyber-ai.org/ 

üìÖ IAPP AI Governance Global ‚Äî September 18‚Äì19, 2025 | Boston, MA, USA | https://iapp.org/conference/iapp-ai-governance-global-north-america/ | IAPP 

üìÖ HackAICon 2025 ‚Äî September 25, 2025 | LX Factory, Lisbon | https://hackaicon.ethiack.com/ | ETHIACK 

üìÖ The AI Summit at Security Education Conference Toronto (SecTor) 2025 ‚Äî September 30, 2025 | MTCC, Toronto, Ontario, Canada | https://www.blackhat.com/sector/2025/ai-summit.html | Black Hat Events | Security Education Conference Toronto (SecTor) 

üìÖ Offensive AI Con ‚Äî October 5‚Äì8, 2025 | Oceanside (San Diego), CA, USA | https://www.offensiveaicon.com/ | Offensive AI Con 

üìÖ AI Agent Security Summit ‚Äî October 8, 2025 | Commonwealth Club, San Francisco, CA, USA | https://zenity.io/resources/events/ai-agent-security-summit-2025 | Zenity 

üìÖ AI Village @ c0c0n ‚Äî October 10‚Äì11, 2025 | Grand Hyatt, Kochi, India | https://c0c0n.org/AI-village | AI Village | c0c0n 

üìÖ GameSec 2025 ‚Äî Game Theory & AI for Security ‚Äî October 13‚Äì15, 2025 | Athens, Greece | https://www.gamesec-conf.org/ 

üìÖ AI Village @ Swiss Cyber Storm ‚Äî October 28, 2025 | Kursaal Bern, Switzerland | https://www.swisscyberstorm.com/ai-village/ |Swiss Cyber Storm 

üìÖ IAPP Privacy. Security. Risk. 2025 ‚Äî October 30‚Äì31, 2025 | San Diego, CA, USA | https://iapp.org/conference/iapp-privacy-security-risk/ | IAPP

üìÖ SANS Fall Cyber Solutions Fest ‚Äî AI Track ‚Äî November 6, 2025 (8:00 AM‚Äì4:00 PM EST) | Virtual | https://www.sans.org/webcasts/fall-cyber-solutions-fest-2025-ai-track | SANS Institute 

üìÖ AI Security Summit 2025 ‚Äî November 12‚Äì13, 2025 | London, UK | https://www.securitysummit.ai/ | AI Security Summit 2025 

üìÖ AI Security Summit @ Black Hat Europe ‚Äî December 9, 2025 | ExCeL London, UK | https://www.blackhat.com/eu-25/ai-summit.html | Black Hat  

üìÖ AI Hacking Village @ BSidesTLV ‚Äî December 11, 2025 | Tel Aviv University, Tel Aviv, Israel | https://bsidestlv.com/ | BSidesTLV 

# üìö Research

üìñ [DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems](https://www.arxiv.org/pdf/2508.01997) - The rapid advancement and widespread adoption of generative artificial intelligence (AI) pose significant threats to the integrity of personal identity, including digital cloning, sophisticated impersonation, and the unauthorized monetization of identity-related data. Mitigating these risks necessitates the development of robust AI-generated content detection systems, enhanced legal frameworks, and ethical guidelines. This paper introduces the Digital Identity Rights Framework (DIRF), a structured security and governance model designed to protect behavioral, biometric, and personality-based digital likeness attributes to address this critical need. By Hammad A., Muhammad Zeeshan Baig, Yasir Mehmood, Nadeem Shahzad, MCSE, MCDBA, CSRP, LION, Ken Huang, MUHAMMAD AZIZ UL HAQ, Muhammad Awais, Ahmed Kamal, Tony Greenberg - https://www.arxiv.org/pdf/2508.01997

üìñ [LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres](https://arxiv.org/pdf/2508.18947) - The integration of LLMs into SOCs presents a transformative, yet still evolving, opportunity to reduce analyst workload through human-AI collaboration. However, their real-world application in SOCs remains underexplored. To address this gap, we present a longitudinal study of 3,090 analyst queries from 45 SOC analysts over 10 months. Our analysis reveals that analysts use LLMs as on-demand aids for sensemaking and contextbuilding, rather than for making high-stakes determinations, preserving analyst decision authority. By Ronal Singh, Shahroz Tariq, PhD, Fatemeh Jalalvand, Mohan Baruwal Chhetri, Surya Nepal, Ph.D., C√©cile Paris, Martin Lochner - https://arxiv.org/pdf/2508.18947
 
üìñ [School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs](https://arxiv.org/pdf/2508.17511) - Reward hacking--where agents exploit flaws in imperfect reward functions rather than performing tasks as intended--poses risks for AI alignment. Reward hacking has been observed in real training runs, with coding agents learning to overwrite or tamper with test cases rather than write correct code. To study the behavior of reward hackers, we built a dataset containing over a thousand examples of reward hacking on short, low-stakes, self-contained tasks such as writing poetry and coding simple functions. We used supervised fine-tuning to train models (GPT-4.1, GPT-4.1-mini, Qwen3-32B, Qwen3-8B) to reward hack on these tasks. By Mia Taylor, James Chua, Jan Betley, Johannes Treutlein, Owain Evans - https://arxiv.org/pdf/2508.17511
 
üìñ [PentestJudge: Judging Agent Behavior Against Operational Requirements](https://arxiv.org/pdf/2508.02921) - Introducing PentestJudge, an LLM-as-judge system for evaluating the operations of pentesting agents. The scores are compared to human domain experts as a ground-truth reference, allowing us to compare their relative performance with standard binary classification metrics, such as F1 scores. By Shane Caldwell, Max Harley, Michael Kouremetis, Vincent Abruzzo at Dreadnode - https://arxiv.org/pdf/2508.02921
 
üìñ [Incident Analysis for AI Agents](https://arxiv.org/pdf/2508.14231v1) - As AI agents become more widely deployed, we are likely to see an increasing number of incidents: events involving AI agent use that directly or indirectly cause harm. For example, agents could be prompt-injected to exfiltrate private information or make unauthorized purchases. Structured information about such incidents (e.g., user prompts) can help us understand their causes and prevent future occurrences. However, existing incident reporting processes are not sufficient for understanding agent incidents. In particular, such processes are largely based on publicly available data, which excludes useful, but potentially sensitive, information such as an agent‚Äôs chain of thought or browser history. By Carson E., Xavier Roberts-Gaal, Alan Chan - https://arxiv.org/pdf/2508.14231v1 
 
üìñ [On the Security and Privacy of Federated Learning: A Survey with Attacks, Defenses, Frameworks, Applications, and Future Directions](https://www.arxiv.org/abs/2508.13730) - Federated Learning (FL) is an emerging distributed machine learning paradigm enabling multiple clients to train a global model collaboratively without sharing their raw data. While FL enhances data privacy by design, it remains vulnerable to various security and privacy threats. This survey provides a comprehensive overview of more than 200 papers regarding the state-of-the-art attacks and defense mechanisms developed to address these challenges, categorizing them into security-enhancing and privacy-preserving techniques. By Daniel Jimenez Martinez, Yelizaveta Falkouskaya, Jose Luis Hernandez, Aris Anagnostopoulos , ioannis chatzigiannakis, Andrea Vitaletti https://www.arxiv.org/abs/2508.13730
 
üìñ [A Guide to Stakeholder Analysis for Cybersecurity Researchers](https://arxiv.org/pdf/2508.14796) - Stakeholder-based ethics analysis is now a formal requirement for submissions to top cybersecurity research venues. This requirement reflects a growing consensus that cybersecurity researchers must go beyond providing capabilities to anticipating and mitigating the potential harms thereof. However, many cybersecurity researchers may be uncertain about how to proceed in an ethics analysis. In this guide, we provide practical support for that requirement by enumerating stakeholder types and mapping them to common empirical research methods. We also offer worked examples to demonstrate how researchers can identify likely stakeholder exposures in realworld projects - By James C. Davis, Sophie Chen, Huiyun Peng, Paschal C. Amusuo, Kelechi G. Kalu -  https://arxiv.org/pdf/2508.14796 
 
üìñ [OS-HARM: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/pdf/2506.14866) - Computer use agents are LLM-based agents that can directly interact with a graphical user interface, by processing screenshots or accessibility trees. While these systems are gaining popularity, their safety has been largely overlooked, despite the fact that evaluating and understanding their potential for harmful behavior is essential for widespread adoption. To address this gap, we introduce OS-HARM, a new benchmark for measuring safety of computer use agents. Thomas Kuntz, Agatha Duzan, Hao Zhao, Francesco Croce, Zico Kolter, Nicolas Hazard, Maksym Andriushchenko - https://arxiv.org/pdf/2506.14866
 
üìñ [In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/pdf/2508.06249) - Fine-tuning lets practitioners repurpose aligned large language models (LLMs) for new domains, yet recent work reveals emergent misalignment (EMA): Even a small, domain-specific fine-tune can induce harmful behaviors far outside the target domain. Even in the case where model weights are hidden behind a fine-tuning API, this gives attackers inadvertent access to a broadly misaligned model in a way that can be hard to detect from the fine-tuning data alone. We present the first systematic study of in-training safeguards against EMA that are practical for providers who expose fine-tuning via an API. By David Kacz√©r, Magnus J√∏rgenv√•g, Clemens Vetter, Lucie Flek, Florian Mai - https://arxiv.org/pdf/2508.06249
 
üìñ [Improving Google A2A Protocol: Protecting Sensitive Data and Mitigating Unintended Harms in Multi-Agent Systems](https://arxiv.org/pdf/2505.12490v3) - Enterprise-grade evaluations of MCP implementations indicate that without robust enforcement of access control and consent boundaries, attackers may subvert tool endpoints or manipulate contextual payloads to bypass authorization policies. Taken together, these findings reveal a gap between the theoretical design of A2A and its practical resilience against adversarial behavior. While A2A and MCP enable functional interoperability, they do not yet provide sufficient guarantees of confidentiality, integrity, and informed consent for handling sensitive information. This paper addresses this gap by identifying core protocol weaknesses in areas such as token management, authentication strength, scope granularity, and data flow transparency. 

We propose a set of targeted enhancements to improve privacy, security, and user control in agent-mediated communications, and demonstrate their application in a real-world example involving vacation booking. Our proposal incorporates best practices from adjacent fields such as zero-trust architectures and regulatory compliance in financial technologies. By Yedidel Louck, Ariel Stulman, Amit Dvir - https://arxiv.org/pdf/2505.12490v3
 
üìñ [Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/pdf/2508.10880) - Can AI agents with access to sensitive information maintain privacy awareness while interacting with other agents? The future of interpersonal interaction is evolving towards a world where individuals are supported by AI agents acting on their behalf. These agents will not function in isolation; instead, they will collaborate, negotiate, and share information with agents representing others. This shift will introduce novel privacy paradigms that extend beyond conventional large language model (LLM) privacy considerations, such as protecting individual data points during training and safeguarding user queries in cloud-based inference services. By Yanzhe Zhang and Diyi Yang - https://arxiv.org/pdf/2508.10880
 
üìñ [Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/pdf/2508.20643) - In this paper, the team introduces CyberSleuth, an autonomous LLM agent designed for the forensic investigation of realistic web application attacks. CyberSleuth autonomously analyses post-attack evidence (e.g., packet-level traces or application logs) and reconstructs the incident by (i) identifying the targeted application, (ii) determining the exploited vulnerability, i.e., the exact Common Vulnerabilities and Exposures (CVE) used, and (iii) assessing whether the attack succeeded. Beyond simply producing a verdict, CyberSleuth generates structured, human-readable forensic reports to support Security Operations Centre (SOC) analysts in resolving incidents efficiently. By Stefano Fumero, Kai Huang, Matteo Boffa, Danilo Giordano, Marco Mellia, Zied Ben Houidi, Dario Rossi https://arxiv.org/pdf/2508.20643
 
üìñ [Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/pdf/2508.10677) - Effective incident response (IR) is critical for mitigating cyber threats, yet security teams are overwhelmed by alert fatigue, high false-positive rates, and the vast volume of unstructured Cyber Threat Intelligence (CTI) documents. While CTI holds immense potential for enriching security operations, its extensive and fragmented nature makes manual analysis time-consuming and resource-intensive. To bridge this gap, we introduce a novel Retrieval-Augmented Generation (RAG)-based framework that leverages Large Language Models (LLMs) to automate and enhance IR by integrating dynamically retrieved CTI. Our approach introduces a hybrid retrieval mechanism that combines NLP-based similarity searches within a CTI vector database with standardized queries to external CTI platforms, facilitating context-aware enrichment of security alerts. By Amine TELLACHE, Abdelaziz Amara-korba, Amdjed Mokhtari, horea moldovan, Yacine Ghamri-Doudane - https://arxiv.org/pdf/2508.10677
 
üìñ [Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/pdf/2508.10043) - When you add an agent to monitor a network, you also add new ways it can be attacked. Using a 7-layer model (MAESTRO). By Pallavi Zambare, Venkata Nikhil Thanikella, Ying Liu https://arxiv.org/pdf/2508.10043

üìñ [Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/pdf/2508.20444v1) - We propose a novel class of threat‚ÄîRansomware 3.0‚Äî that uses LLMs to orchestrate all phases of its attack chain including autonomous synthesis and deployment of tailored malicious payloads on the fly, adapting to the execution environment and personalizing extortion demands. We examine the feasibility and ramifications of Ransomware 3.0, which invokes an LLM to (i) probe the victim environment, (ii) locate sensitive information, (iii) devise and execute an attack vector, and (iv) generate personalized extortion notes, thereby enacting the entire ransomware campaign with no human operator. By Md Raz, Meet Udeshi, Sai Charan P.V, Prashanth K., FARSHAD KHORRAMI, Ramesh Karri https://arxiv.org/pdf/2508.20444v1
 
üìñ [Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/pdf/2508.06601) - How can open-weight Large Language Models be safeguarded against malicious uses? We explore an intuitive yet understudied question: Can we prevent LLMs from learning unsafe technical capabilities (such as CBRN) by filtering out enough of the relevant pretraining data before we begin training a model? We train multiple 6.9B LLMs from scratch on an unfiltered dataset and on filtered versions where we filtered out biorisk knowledge. By Kyle O‚ÄôBrien, Stephen Casper, Quentin Anthony, Tomek Korbak, Robert Kirk, Xander Davies, Ishan Mishra, Geoffrey Irving, Yarin Gal, Stella Biderman https://arxiv.org/pdf/2508.06601 
 
üìñ [Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous](https://arxiv.org/pdf/2508.12175) - by Ben Nassi, PhD, Stav Cohen, Or Yair https://arxiv.org/pdf/2508.12175 / https://sites.google.com/view/invitation-is-all-you-need/ 


# Tools & Resources

üß∞ [Claude Code Security Review](https://github.com/anthropics/claude-code-security-review) - Prompts and scripts to run code security reviews with Claude Code, producing structured findings and suggested fixes ‚≠êÔ∏è2.2k Anthropic David Dworken https://github.com/anthropics/claude-code-security-review

üß∞ [hexstrike-ai](https://github.com/0x4m4/hexstrike-ai) - HexStrike AI MCP Agents is an advanced MCP server that lets AI agents (Claude, GPT, Copilot, etc.) autonomously run 150+ cybersecurity tools for automated pentesting, vulnerability discovery, bug bounty automation, and security research. Seamlessly bridge LLMs with real-world offensive security capabilities. ‚≠êÔ∏è 1.8k Alistair Oxley and Muhammad Osama - HexStrike AI https://github.com/0x4m4/hexstrike-ai

üß∞ [Beelzebub](https://github.com/mariocandela/beelzebub) - Offensive AI security toolkit and honeypot framework for detecting and analyzing malicious AI agent behavior ‚≠êÔ∏è1.4k Mario Candela https://github.com/mariocandela/beelzebub

üß∞ [AutorizePro](https://github.com/WuliRuler/AutorizePro) - AI-assisted Burp Suite extension for broken access control and authorization testing with lower false positives ‚≠êÔ∏è422 https://github.com/WuliRuler/AutorizePro

üß∞ [Atlantis CRS](https://github.com/Team-Atlanta/aixcc-afc-atlantis) - AIxCC finals Cyber Reasoning System from Team Atlanta ‚≠êÔ∏è332 Georgia Institute of Technology Team Atlanta https://github.com/Team-Atlanta/aixcc-afc-atlantis

üß∞ [Open-Prompt-Injection](https://github.com/liu00222/Open-Prompt-Injection) - Benchmark and tooling for implementing and evaluating prompt injection attacks and defenses in LLM apps ‚≠êÔ∏è268 Yupei Liu https://github.com/liu00222/Open-Prompt-Injection 

üß∞ [Theori AIxCC Finals Archive](https://github.com/theori-io/aixcc-afc-archive) - Public release of Theori‚Äôs AIxCC finals CRS ‚≠êÔ∏è141 Theori https://github.com/theori-io/aixcc-afc-archive

üß∞ [HOUND](https://github.com/muellerberndt/hound) - Language-agnostic AI code security analysis that mirrors expert auditor workflows for automated audits ‚≠êÔ∏è174 Bernhard Mueller https://github.com/muellerberndt/hound

üß∞ [Anamorpher](https://github.com/trailofbits/anamorpher) - Image scaling attack generator and visualizer for multi-modal prompt injection testing ‚≠êÔ∏è115 Trail of Bits https://github.com/trailofbits/anamorpher

üß∞ [HackGpt](https://github.com/yashab-cyber/HackGpt) - LLM-powered offensive security assistant for recon and exploitation workflows ‚≠êÔ∏è72 Yashab Alam https://github.com/yashab-cyber/HackGpt

üß∞ [SHERPA (AIxCyberChallenge)](https://github.com/AIxCyberChallenge/sherpa) - LLM-powered targeted fuzzing and harness generation for attacker-controlled entry points ‚≠êÔ∏è67 https://github.com/AIxCyberChallenge/sherpa

üß∞ [BugTrace-AI](https://github.com/yz9yt/BugTrace-AI) - AI-assisted bug triage and reproduction from logs and traces to accelerate debugging ‚≠êÔ∏è63 https://github.com/yz9yt/BugTrace-AI

üß∞ [Prompt Injector](https://github.com/BlueprintLabIO/prompt-injector) - A minimal TypeScript library with 25+ curated prompt injection patterns from leading security research. ‚≠êÔ∏è38 By BluePrintLAB https://github.com/BlueprintLabIO/prompt-injector

üß∞ [Flagwise](https://github.com/bluewave-labs/flagwise) - Shadow AI detection and LLM traffic monitoring server with analytics and real-time alerting ‚≠êÔ∏è24 BlueWave Labs https://github.com/bluewave-labs/flagwise

üß∞ [Xiangxin Guardrails](https://github.com/xiangxinai/xiangxin-guardrails) - Enterprise-grade AI safety guardrails platform for prompt attack detection and content compliance with on-prem deployment and SDKs ‚≠êÔ∏è23 https://github.com/xiangxinai/xiangxin-guardrails 

üß∞ [aisi-sandboxing](https://github.com/UKGovernmentBEIS/aisi-sandboxing) - The open-source AISI toolkit for sandboxing agentic evaluations ‚≠êÔ∏è13 Jacob A. and Jason Gwartz at AI Security Institute https://github.com/UKGovernmentBEIS/aisi-sandboxing 

üß∞ [AWS iReveal MCP](https://github.com/sysdiglabs/aws-ireveal-mcp) - Sysdig Labs MCP tool to surface AWS identity and resource exposure insights for investigations ‚≠êÔ∏è13 Sysdig https://github.com/sysdiglabs/aws-ireveal-mcp 

üß∞ [BugBuster CRS 42-b3yond-6ug](https://github.com/42-b3yond-6ug/42-b3yond-6ug-crs) - AIxCC finals Cyber Reasoning System ‚≠êÔ∏è11 https://github.com/42-b3yond-6ug/42-b3yond-6ug-crs

üß∞ [RAG Firewall](https://github.com/taladari/rag-firewall) - Client-side retrieval firewall for RAG systems that blocks prompt injection and secret leaks and re-ranks untrusted content ‚≠êÔ∏è7 https://github.com/taladari/rag-firewall 

üß∞ [Phantomwall](https://github.com/c77-source/phantomwall) - Open-source prompt-injection firewall and telemetry to ship AI apps safely in minutes ‚≠êÔ∏è3 https://github.com/c77-source/phantomwall

# üé• Videos
‚ñ∂Ô∏è The Man Who Might SOLVE AI Alignment - by Steven Byrnes at Astera Institute 

‚ñ∂Ô∏è Building Secure ReactJS Apps: Mastering Advanced Security Techniques - by Jim Manico at Manicode Security 

‚ñ∂Ô∏è Goodbye SOC Analyst, Hello Cyber Orchestration Engineer - by Josh Madakor at LOG(N) Pacific 

‚ñ∂Ô∏è Eyes in the Sky | Inside the Defense Department's Secretive Imagery Agency - by Catherine Herridge 

‚ñ∂Ô∏è DEF CON 33 Video Team - AIxCC with Shellphish 

‚ñ∂Ô∏è DEF CON 33 Video Team - AIxCC: 42 Beyond Bugs - by DEF CON 

‚ñ∂Ô∏è Inside the AIxCC Retrospective | Andrew Carney at Defense Advanced Research Projects Agency (DARPA)  

‚ñ∂Ô∏è (Mis)adventures with Copilot+: Attacking and Exploiting Windows NPU Drivers - by Nicola Stauffer and  G√ºrkan G√úR at ZHAW Institute of Computer Science (InIT) 

‚ñ∂Ô∏è The Pivotal Role of Large Language Models in Extracting Actionable TTP Attack Chains - by Jack Tang; Lorin Wu; Porot Mo at Qihoo 360 

‚ñ∂Ô∏è The Oversights Under the Flow: Discovering the Vulnerable Tooling Suites From Azure MLOps - by Peng Zhou at Shanghai University 

‚ñ∂Ô∏è Utilizing AI Models to Conceal and Extract Commands in C2 Images - by Feng Q. and Chris Navarrete at Palo Alto Networks 

‚ñ∂Ô∏è AI Model Penetration: Testing LLMs for Prompt Injection & Jailbreaks - by Jeff Crume, PhD, CISSP  at IBM Research 


ü§ù Let‚Äôs Connect
If you‚Äôre a founder building something new or an investor evaluating early-stage opportunities‚Ää‚Äî‚Äälet‚Äôs connect.


üí¨ Read something interesting? Share your thoughts in the comments.
