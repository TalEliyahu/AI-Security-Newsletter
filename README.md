# AI Security Newsletterâ€Š-â€ŠJanuary 2026

A digest of AI security research, insights, reports, upcoming events, and tools & resources. Follow the [AISecHub community](https://x.com/AISecHub) and our [LinkedIn group](https://www.linkedin.com/groups/14545517/) for additional updates. Also check out our project, [Awesome AI Security](https://github.com/TalEliyahu/Awesome-AI-Security).

Sponsored by [InnovGuard.com](https://innovguard.com) - Technology Risk & Cybersecurity Advisory - Innovate and Invest with Confidence, Lead with Assurance.

---

# ğŸ” Insights

ğŸ“Œ [ZombieAgent: New ChatGPT Vulnerabilities Let Data Theft Continue and Spread](https://www.radware.com/blog/threat-intelligence/zombieagent/)  
Radware's Zvika Babo breaks down "ZombieAgent" abuse paths where prompt injection + agent memory / tool access / connectors enable persistent data exposure, lateral spread across sessions, and follow-on abuse through connected apps and agent workflows.

ğŸ“Œ [OWASP AI Security Guide](https://owaspai.org)  
Free access to 300+ pages of practical guidance on protecting AI and data-centric systems - as contributed to the AI Act and ISO 27090 through a unique liaison partnership.

ğŸ“Œ [Personal AI Agents like Moltbot Are a Security Nightmare](https://blogs.cisco.com/ai/personal-ai-agents-like-moltbot-are-a-security-nightmare)  
Amy Chang and Vineeth Sai Narajala explain why Moltbot's "skills" model is dangerous: it can execute local commands, touch files, and leak credentials, and malicious skills can hide prompt-injection and data-exfiltration steps - so Cisco released the open-source Skill Scanner to flag risky skill content before it's installed.

ğŸ“Œ [OWASP Agentic AI Top 10: Threats in the Wild](https://labs.lares.com/owasp-agentic-top-10)  
RaÃºl Redondo maps real-world agent failures to the OWASP Agentic AI Top 10, showing how prompt injection, tool misuse, insecure data flows, and weak isolation show up in practical deployments, not just theory.

ğŸ“Œ [Coding Agents: The Insider Threat You Installed Yourself](https://blog.securitybreak.io/coding-agents-the-insider-threat-you-installed-yourself-35644a1d5409)  
Thomas Roccia frames coding agents as "pre-installed insiders" where excessive repo access, secrets exposure, and auto-execution can turn normal dev workflows into high-impact compromise paths without classic malware delivery.

ğŸ“Œ [AI Security Challenges in 2026](https://hi120ki.github.io/blog/posts/20260103/)  
The year 2025 witnessed a continuous cycle of emerging and evolving AI/LLM technologies. Across the industry, various security measures for AI have been advancing. In 2026, AI adoption is expected to expand further, bringing new technologies and demanding corresponding security measures. This article reviews the major topics from 2025, breaks down the anticipated industry-wide challenges in AI Security for 2026 into concrete action items, and summarizes security measures for safely using and providing AI. - Hiroki Akamatsu

ğŸ“Œ [BodySnatcher: Broken Authentication and Agentic Hijacking in ServiceNow](https://appomni.com/ao-labs/bodysnatcher-agentic-ai-security-vulnerability-in-servicenow)  
AppOmni's Aaron C. details how a ServiceNow Virtual Agent API + Now Assist AI Agents flaw enabled unauthenticated impersonation using only an email address, bypassing MFA/SSO assumptions and allowing hijacked agent workflows to access data and actions as the victim.

ğŸ“Œ [IBM AI ("Bob") Downloads and Executes Malware](https://www.promptarmor.com/resources/ibm-ai-%28-bob-%29-downloads-and-executes-malware)  
PromptArmor walks through a concrete failure mode where prompt injection can push an agent into download + execute when users enable auto-approve / "always allow" for tool actions (even a seemingly safe one like echo), highlighting why execute chains need strict allowlists, sandboxing, and policy enforcement.

ğŸ“Œ [Cyber Toolkits Update: Models Are Getting Better at Finding and Exploiting Vulns on Realistic Ranges](https://red.anthropic.com/2026/cyber-toolkits-update/)  
Brian S. summarizes red-team observations that capability is shifting from toy CTFs to more realistic cyber ranges, implying higher baseline exploit discovery risk as models gain planning depth and tool proficiency.

ğŸ“Œ [ChainLeak: Critical AI Framework Vulnerabilities Expose Data, Enable Cloud Takeover](https://www.zafran.io/resources/chainleak-critical-ai-framework-vulnerabilities-expose-data-enable-cloud-takeover)  
Gal Zaban and Ido Shani describe Chainlit vulnerabilities where insecure defaults/misconfig and exposed interfaces can lead to cloud API key leakage and sensitive file theft (CVE-2026-22218) plus SSRF (CVE-2026-22219) - which can expand into broader cloud compromise depending on what's reachable.

ğŸ“Œ [Exploiting LLM Write Primitives: System Prompt Extraction When Chat Output Is Locked Down](https://www.praetorian.com/blog/exploiting-llm-write-primitives-system-prompt-extraction-when-chat-output-is-locked-down/)  
Winston H. shows how "write primitives" and constrained-output environments can still be coerced into leaking system prompts via indirect channels, formatting constraints, and output-lock bypass patterns.

ğŸ“Œ [The ServiceNow AI Vulnerability: What Went Wrong and How to Secure Your AI Agents](https://opena2a.org/blogs/servicenow-ai-vulnerability)  
Abdel Fane reframes the ServiceNow incident as an agent security design problem: least privilege on actions, strict tool authorization, robust identity boundaries, and continuous monitoring for anomalous agent behavior.

ğŸ“Œ [Reprompt: The Single-Click Microsoft Copilot Attack That Silently Steals Your Personal Data](https://www.varonis.com/blog/reprompt)  
Dolev Taler explains a "single-click" Copilot exploitation flow where crafted content triggers unsafe retrieval/disclosure behaviors, emphasizing that UX-level interactions can be enough to trigger high-impact exfiltration.

ğŸ“Œ [How We Found Code Execution in Anthropic's Official Git MCP Server](https://cyata.ai/blog/cyata-research-breaking-anthropics-official-mcp-server)  
Cyata's Yarden Porat details an RCE-class issue in an MCP server context, showing how "official" integrations can still expand the attack surface through input handling, plugin logic, and trust assumptions around tool servers.

ğŸ“Œ [Achieving Remote Code Execution on n8n Via Sandbox Escape - CVE-2026-1470 & CVE-2026-0863](https://research.jfrog.com/post/achieving-remote-code-execution-on-n8n-via-sandbox-escape/)  
Natan Nehorai describes a sandbox escape chain leading to RCE in n8n, illustrating how isolation layers fail in practice when combined with deserialization, escaping primitives, or weak boundary controls.

ğŸ“Œ [NI8MARE: Unauthenticated Remote Code Execution in n8n (CVE-2026-21858, CVSS 10.0)](https://www.cyera.com/research-labs/ni8mare-unauthenticated-remote-code-execution-in-n8n-cve-2026-21858)  
Dor Attias reports an unauthenticated RCE path to full takeover of exposed n8n instances, a reminder that low-friction automation platforms become high-value targets when Internet-facing and misconfigured.

ğŸ“Œ [The Hidden Backdoor in Claude Code: Why Its Power Is Also Its Greatest Vulnerability](https://www.lasso.security/blog/the-hidden-backdoor-in-claude-coding-assistant)  
Or Oxenberg and Eliran Suisa argue that coding assistants amplify insider-style risk: they sit inside privileged dev contexts, touch secrets and repos, and can be influenced by poisoned context, dependencies, or instructions.

ğŸ“Œ [Securing Agents in Production (Agentic Runtime, #1)](https://blog.palantir.com/securing-agents-in-production-agentic-runtime-1-5191a0715240)  
Palantir outlines an "agentic runtime" mindset for production: treat agent actions like code execution, enforce policies at the action layer, record full traces, and design for containment when the model behaves unexpectedly.

ğŸ“Œ [KONNI Adopts AI to Generate PowerShell Backdoors](https://research.checkpoint.com/2026/konni-targets-developers-with-ai-malware/)  
From Check Point Research, this post shows threat actors using GenAI to accelerate scripting and payload development, lowering the barrier to producing varied PowerShell backdoors and social engineering content.

ğŸ“Œ [Supply-Chain Risk of Agentic AI: Infecting Infrastructures via Skill Worms](https://blog.lukaszolejnik.com/supply-chain-risk-of-agentic-ai-infecting-infrastructures-via-skill-worms/)  
Lukasz Olejnik, Ph.D, LL.M discusses "skill worms" as a propagation mechanism in agent ecosystems, where compromised skills, plugins, or tool bundles can spread across organizations through reuse, marketplaces, and inherited trust.

---

# ğŸ§° Tools & Resources

ğŸ§° **[skills](https://github.com/trailofbits/skills)** - Claude Code skills marketplace from Trail of Bits for security research, vuln detection, and audit workflows (includes insecure-defaults plugin for insecure defaults + hardcoded creds). â­ï¸2.2k Dan Guido

ğŸ§° **[anamnesis-release](https://github.com/SeanHeelan/anamnesis-release)** - Evaluation framework for testing how LLM agents generate exploits from bug reports under exploit mitigations. â­ï¸479 Sean Heelan

ğŸ§° **[awesome-dfir-skills](https://github.com/tsale/awesome-dfir-skills)** - Community library of DFIR skills, prompts, workflows, and helpers for faster, consistent incident response. â­ï¸240 Thomas Roccia

ğŸ§° **[Burp AI Agent](https://github.com/six2dez/burp-ai-agent)** - Burp Suite extension adding MCP tooling plus AI-assisted analysis, privacy controls, and passive/active scanning. â­ï¸221 Alexis FernÃ¡ndez

ğŸ§° **[vulnerable-mcp-servers-lab](https://github.com/appsecco/vulnerable-mcp-servers-lab)** - Intentionally vulnerable MCP server implementations to learn, demo, and practice exploiting MCP flaws. â­ï¸216 Riyaz Walikar

ğŸ§° **[OpenRT](https://github.com/AI45Lab/OpenRT)** - Open-source red teaming framework for multimodal LLMs. â­ï¸216

ğŸ§° **[Security-Detections-MCP](https://github.com/MHaggis/Security-Detections-MCP)** - MCP server for querying Sigma, Splunk ESCU, Elastic, and KQL detection rules from one database. â­ï¸173 Michael H.

ğŸ§° **[Vulnhalla](https://github.com/cyberark/Vulnhalla)** - Runs CodeQL on GitHub databases, adds code context, and uses LLM-guided review to reduce false positives. â­ï¸163

ğŸ§° **[medusa](https://github.com/Pantheon-Security/medusa)** - AI-first security scanner with analyzers + agent rules, focused on reducing false positives and CVE detection. â­ï¸122

ğŸ§° **[a2a-scanner](https://github.com/cisco-ai-defense/a2a-scanner)** - Security scanner for Agent-to-Agent protocol implementations using static analysis, runtime monitoring, and semantic detection. â­ï¸105 Vineeth Sai Narajala

ğŸ§° **[ai_for_the_win](https://github.com/depalmar/ai_for_the_win)** - Hands-on labs for building AI security tools across ML, LLMs, RAG, DFIR, and red teaming. â­ï¸72

ğŸ§° **[GitHub Security Lab Taskflow Agent](https://github.com/GitHubSecurityLab/seclab-taskflow-agent)** - Open, collaborative framework from GitHub Security Lab for AI-assisted security research workflows. â­ï¸59

ğŸ§° **[Skill Scanner](https://github.com/cisco-ai-defense/skill-scanner)** - Security scanner for agent skills (prompt injection, data exfil, malicious code) via rules + LLM-as-judge + behavioral dataflow; SARIF + plugins. â­ï¸59 Cisco

ğŸ§° **[MCP-Dandan](https://github.com/82ch/MCP-Dandan)** - Desktop monitoring tool that observes MCP traffic and flags threats in real time. â­ï¸57

ğŸ§° **[syd](https://github.com/Sydsec/syd)** - Air-gapped pentest assistant that analyzes scan outputs with local LLMs and RAG, no internet required. â­ï¸52

ğŸ§° **[ToolSafe](https://github.com/MurrayTom/ToolSafe)** - Safer agent tool use via step-level guardrails, monitoring, and feedback-driven reasoning. â­ï¸32

ğŸ§° **[ai-soc-agent](https://github.com/M507/ai-soc-agent)** - AI SOC investigation platform using MCP for case management, SIEM analysis, and CTI enrichment. â­ï¸10

ğŸ§° **[promptshield](https://github.com/Neural-alchemy/promptshield)** - Framework to protect LLM apps from prompt injection and jailbreaks across common providers and stacks. â­ï¸4

ğŸ§° **[mcp-fortress](https://github.com/mcp-fortress/mcp-fortress)** - MCP scanner plus runtime protection layer to detect and block unsafe MCP behavior. â­ï¸2

ğŸ§° **[promptxploit](https://github.com/Neural-alchemy/promptxploit)** - Security testing framework for finding LLM application vulnerabilities before deployment. â­ï¸2

ğŸ§° **[lockllm-npm](https://github.com/lockllm/lockllm-npm)** - Official JS/TS SDK for LockLLM runtime protection (prompt injection/hidden instructions/data exfil detection). â­ï¸1

ğŸ§° **[BlackIce](https://hub.docker.com/r/databricksruntime/blackice)** - Ready-to-run Docker image for AI security red teaming (Kali-inspired), avoids tool setup pain and dependency conflicts. â­ï¸ [Databricks blog](https://databricks.com/blog/announcing-blackice-containerized-red-teaming-toolkit-ai-security-testing) | [Paper](https://arxiv.org/abs/2510.11823)

---

# ğŸ“„ Reports

ğŸ“˜ **[The State of Non-Human Identity and AI Security](https://www.linkedin.com/feed/update/urn:li:activity:7422210133281304576)**  
CSA Oasis survey Aug-Sep 2025 sample 383 finds orgs treat AI identities like NHIs such as service accounts, API keys, chatbots, inheriting credential sprawl and unclear ownership. Biggest gaps include 51% no clear owner, 51% over permissioned access, 49% low visibility, 46% stale or orphaned IDs. Token control lags with 16% not tracking creation and 24% needing over 24h to rotate or revoke after exposure.

ğŸ“˜ **[Securing Artificial Intelligence Baseline Cyber Security Requirements for AI Models and Systems](https://www.linkedin.com/feed/update/urn:li:activity:7418008087607001089)**  
European standard defining baseline cyber security requirements for AI systems including deep neural networks and generative AI. Organizes 13 principles across the lifecycle secure design, development, deployment, maintenance, end of life. Covers threat awareness, risk assessment and threat modelling, least privilege permissions, asset inventory, secure infrastructure and supply chain, documentation of data models prompts, testing, incident readiness, monitoring, updates, secure disposal.

ğŸ“˜ **[Deepfakes: Toolkit for Schools and Parents](https://www.linkedin.com/feed/update/urn:li:activity:7416774408511463424)**  
Hong Kong's Office of the Privacy Commissioner for Personal Data (PCPD) has published guidance on the use of an AI deepfake protection toolkit aimed at schools and parents. The guidance explains common types of deepfakes and typical scenarios involving abusive deepfakes in school settings, focusing on risks faced by students. The toolkit provides practical measures for prevention and incident response, outlining the roles of schools, parents, and students. Recommended school-level safeguards include data minimization, restricting access to personal data, and implementing general data security measures to reduce exposure to deepfake misuse.

ğŸ“˜ **[Securing Agentic AI](https://www.linkedin.com/feed/update/urn:li:activity:7412047391094181888)**  
CSA Singapore draft for public consultation addendum to Securing AI Systems guide targets agentic AI systems that plan and act across steps. Practical controls cover risk assessment hardening and supply chain, asset and secret hygiene, authentication authorisation, limiting agency, segmentation, secure MCP and inter agent comms, monitoring logging, human in loop, and vulnerability disclosure.

ğŸ“˜ **[Global Cybersecurity Outlook 2026](https://www.linkedin.com/feed/update/urn:li:activity:7416787754149171200)**  
The World Economic Forum's Global Cybersecurity Outlook 2026 report explores how accelerating AI adoption, geopolitical fragmentation and widening cyber inequity are reshaping the global risk landscape. As attacks grow faster, more complex and more unevenly distributed, organizations and governments face rising pressure to adapt amid persistent sovereignty challenges and widening capability gaps. Drawing on leaders' perspectives, the report provides actionable insights to inform strategy, investment and policy.

---

# ğŸ“… Upcoming Conferences

## February 2026

ğŸ“… [DiCyFor & AI Security Summit (Singapore)](https://www.dicyfor.com/singapore2026) - February 10, 2026 Â· Singapore Â· Organizer: DiCyFor  
ğŸ“… [IEEE ICAIC 2026 - International Conference on AI in Cybersecurity](https://icaic.gyancity.com/) - February 18-20, 2026 Â· University of Houston, Houston, TX, USA Â· Organizer: IEEE / ICAIC

## March 2026

ğŸ“… [[un]prompted - The AI Security Practitioner Conference](https://unpromptedcon.org/) - March 3-4, 2026 Â· Salesforce Tower, San Francisco, CA, USA Â· Organizer: [un]prompted  
ğŸ“… [AI Security Summit 2026](https://events.lynx.co/ai-security-summit/) - March 10, 2026 Â· Check Point HQ, Tel Aviv, Israel Â· Organizer: Lynx Events  
ğŸ“… [DiCyFor & AI Security Summit (Bangkok)](https://www.dicyfor.com/bangkok2026) - March 11-12, 2026 Â· Bangkok, Thailand Â· Organizer: DiCyFor  
ğŸ“… [IEEE SaTML 2026 - Secure and Trustworthy Machine Learning](https://satml.org/) - March 23-25, 2026 Â· Munich, Germany Â· Organizer: IEEE SaTML

## April 2026

ğŸ“… [DiCyFor & AI Security Summit (Kuala Lumpur)](https://www.dicyfor.com/kualalumpur2026) - April 15, 2026 Â· Kuala Lumpur, Malaysia Â· Organizer: DiCyFor  
ğŸ“… [SANS AI Cybersecurity Summit 2026](https://www.sans.org/cyber-security-training-events/ai-summit-2026) - April 20-21, 2026 Â· Arlington, VA, USA & Virtual Â· Organizer: SANS Institute  
ğŸ“… [AI Security Summit @ Black Hat Asia](https://www.blackhat.com/asia-26/ai-security-summit.html) - April 22, 2026 Â· Marina Bay Sands, Singapore Â· Organizer: BlackHat

---

# ğŸ“š Research

ğŸ“– **The Promptware Kill Chain: How Prompt Injections Gradually Evolved Into a Multi-Step Malware**  
Ben Nassi, PhD, Bruce Schneier, Oleg Brodt. [arXiv](https://arxiv.org/abs/2601.09625)

ğŸ“– **When Bots Take the Bait: Exposing and Mitigating the Emerging Social Engineering Attack in Web Automation Agent**  
Xinyi W., Hongshan Geng, Yueyue C., Mingxuan L., é™è²å„¿, PAN Xudong, Jiarun Dai, Baojun Liu [PDF](https://arxiv.org/pdf/2601.07263)

ğŸ“– **SecMLOps: A Comprehensive Framework for Integrating Security Throughout the Machine Learning Operations Lifecycle**  
Xinrui (Michaela) Z., Pincan Zhao, Jason Jaskolka, Heng Li, Rongxing Lu [PDF](https://arxiv.org/pdf/2601.10848)

ğŸ“– **System-level Security for Computer Use Agents**  
Hanna Foerster, Robert Mullins, Tom Blanchard, Nicolas Papernot, Kristina NikoliÄ‡, Florian TramÃ¨r, Ilia Shumailov, å¼ ç¨‹, Yiren Z. [PDF](https://arxiv.org/pdf/2601.09923)

ğŸ“– **Agent Skills in the Wild: An Empirical Study of Security Vulnerabilities at Scale**  
Yi Liu, Weizhe Wang, Ruitao Feng, Yao Zhang, Guangquan Xu, Gelei Deng, Yuekang Li, Leo Zhang [PDF](https://arxiv.org/pdf/2601.10338)

ğŸ“– **FinVault: Benchmarking Financial Agent Safety in Execution-Grounded Environments**  
Zhi Yang, runguo li, å¼ºçªçª, Jiashun Wang, å¨„æ–¹æ·‡, MENGPING LI, ç¨‹ä¸œå¡, Rui Xu, Heng Lian, shuo zhang, XiaoLong Liang, Xiaoming Huang, John Wei Zheng, Zhaowei Liu, Xin Guo, Huacan Wang, RongHao Chen, Liwen ZHANG [PDF](https://arxiv.org/pdf/2601.07853)

ğŸ“– **Cybersecurity AI: A Game-Theoretic AI for Guiding Attack and Defense**  
VÃ­ctor Mayoral-Vilches, PhD, MarÃ­a Sanz GÃ³mez, Francesco Balassone, Stefan Rass, Lidia Salas-Espejo, Benjamin Jablonski, Luis Javier Navarete, Maite del Mundo, CristÃ³bal R. J. Veas Chavez [PDF](https://arxiv.org/pdf/2601.05887)

ğŸ“– **HoneyTrap: Deceiving Large Language Model Attackers to Honeypot Traps with Resilient Multi-Agent Defense**  
Siyuan Li, Xi Lin, Jun W., Zehao Liu, Haoyu Li, é å¤©æ°, Xiang Chen, jianhua liu [arXiv](https://arxiv.org/abs/2601.04034)

ğŸ“– **Multi-Agent Framework for Threat Mitigation and Resilience in AI-Based Systems**  
Armstrong Foundjem, Lionel T., Ph.D, LÃ©uson Da Silva, Foutse Khomh [PDF](https://arxiv.org/pdf/2512.23132)

ğŸ“– **It's a TRAP! Task-Redirecting Agent Persuasion Benchmark for Web Agents**  
Karolina Korgul, Yushi Yang, Arkadiusz Drohomirecki, Piotr BÅ‚aszczyk, Will Howards, Lukas Aichberger, Chris Russell, Ethan (Philip) H. SEOW, è§é¸¿ä¸š, Adel Bibi [PDF](https://arxiv.org/pdf/2512.23128)

---

# ğŸ¥ Videos

1ï¸âƒ£ When Vibe Scammers Met Vibe Hackers: Pwning PhaaS with Their Own Weapons - Chiao-Lin Yu (Steven Meow) at Trend Micro  
2ï¸âƒ£ How to Build an AI Security Program from Scratch - Shannon Murphy at TrendAI  
3ï¸âƒ£ Security AI Agent! Automated Penetration Testing - Loi Liang Yang  
4ï¸âƒ£ AI-Generated Malware: Ireland Legalizes Spyware - Steve Gibson and Leo Laporte at TWiT  
5ï¸âƒ£ AI and Its Impact on Offensive Security Roles in 2026 - Stephen Sims at Off By One Security  
6ï¸âƒ£ MCP with .NET: Securely Exposing Your Data to LLMs - Callum Whyte at Bump  
7ï¸âƒ£ AI Red Teaming: What Breaks, How It Breaks, and the Human Role - rez0  
8ï¸âƒ£ Exploiting AI: A Case Study on Voice Biometric Penetration Testing - Skyler Tuter at TrustedSec  
9ï¸âƒ£ Stochastic Garrotes: A Data-Driven Approach to LLM-Generated Malware - Ryan Ashley at IQT Labs  
ğŸ”Ÿ Backdooring LLMs and Bypassing HuggingFace Malware Scanners - Davide Cioccia at DCODX  
1ï¸âƒ£1ï¸âƒ£ Building a Practical AI Assistant for Security Operations - Vincent Ruijter at Sourcegraph  
1ï¸âƒ£2ï¸âƒ£ GenAI Agentic Security in Practice - Black Hat Europe AI Summit  
1ï¸âƒ£3ï¸âƒ£ The AI Triple Security Gap: Why Your Gateway Strategy Is Already Obsolete - Carlos VillanÃºa FernÃ¡ndez at Traefik Labs  
1ï¸âƒ£4ï¸âƒ£ The Three Horsemen of the AppSec AI-pocalypse - Adam Krieger at Online Business Systems  
1ï¸âƒ£5ï¸âƒ£ Practical Automation of Penetration Testing with Agentic AI - Hiroaki Toyota at LAC  
1ï¸âƒ£6ï¸âƒ£ Building Secure AI Applications with the OWASP Top 10 - Gavin Klondike at GlitchSecure  
1ï¸âƒ£7ï¸âƒ£ AI Agents with Gemini 2.0: Beyond the Chatbot - MÃ¡rton Kodok at Google  
1ï¸âƒ£8ï¸âƒ£ Why Should We Be Careful with AI? - Maciej Krzysica at j-labs  
1ï¸âƒ£9ï¸âƒ£ Panel: The Present and Future of AI and Security - David Brumley and panel  
2ï¸âƒ£0ï¸âƒ£ Governance and Security of APIs and MCPs - Isabelle Mauny at WSO2  
2ï¸âƒ£1ï¸âƒ£ How I Used and Abused LLMs to Get Top 250 on HTB - Rambo Anderson-You  
2ï¸âƒ£2ï¸âƒ£ Securing the AI Revolution - FranÃ§ois  
2ï¸âƒ£3ï¸âƒ£ MCP: Making Compromise Possible - AI Workflows and Security Implications - Nathan Getty at Menlo Security  
2ï¸âƒ£4ï¸âƒ£ AI Is Undermining Our Privacy: What Can We Do About It? - Robert Stribley at Technique

---

# ğŸ¤ Let's Connect
If you're a founder building something new or an investor evaluating early-stage opportunities - [let's connect](https://calendly.com/innovguard/meeting).

ğŸ’¬ Read something interesting? Share your thoughts in the comments.

