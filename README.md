# AI Security Digest – June 2025

A digest of AI security research, insights, reports, upcoming events, and tools & resources. Follow the [AI Security community](https://linktr.ee/AISECHUB) on [Twitter](https://twitter.com) and [LinkedIn group](https://linkedin.com) for additional updates.

Sponsored by [CyberBiz](https://cyber-biz.com/) and [InnovGuard.com](https://innovguard.com) – Technology Risk & Cybersecurity Advisory, Innovate and Invest with Confidence, Lead with Assurance.

## 🔍 Insights

📌 Mapping: MAESTRO Threats — MITRE D3FEND Techniques — This website presents an interactive exploration of the intersection between two pivotal cybersecurity frameworks: MAESTRO and MITRE D3FEND. It aims to provide cybersecurity professionals with actionable insights into securing Agentic AI systems by mapping identified threats to corresponding defensive techniques, by Edward Lee. https://edward-playground.github.io/maestro-d3fend-mapping/

📌 10 Key Risks of Shadow AI — A practical breakdown of Shadow AI: how unmanaged AI use — including tools, models, and features — creates hidden risks across security, compliance, data, and governance. https://www.linkedin.com/pulse/10-key-risks-shadow-ai-tal-eliyahu-9aopc/

📌 How an AI Agent Vulnerability in LangSmith Could Lead to Stolen API Keys and Hijacked LLM Responses - by Sasi Levi and Gal Moyal, Noma Security - https://noma.security/blog/how-an-ai-agent-vulnerability-in-langsmith-could-lead-to-stolen-api-keys-and-hijacked-llm-responses/

📌 Explore the latest threats to Model Context Protocol (MCP) — covering issues from prompt injection to agent hijacking — in this digest collected by Adversa AI. https://adversa.ai/blog/mcp-security-digest-june-2025/

📌 GenAI Guardrails: Implementation & Best Practices —  Lasso outlines how organizations are designing and deploying guardrails for generative AI — including challenges, frameworks, and real-world examples. https://www.lasso.security/blog/genai-guardrails

📌 Trend Micro’s “Unveiling AI Agent Vulnerabilities” 4-part series explores key security threats in agentic AI systems — including [Part I: Introduction](https://www.trendmicro.com/vinfo/us/security/news/threat-landscape/unveiling-ai-agent-vulnerabilities-part-i-introduction-to-ai-agent-vulnerabilities), [Part II: Code Execution](https://www.trendmicro.com/vinfo/us/security/news/cybercrime-and-digital-threats/unveiling-ai-agent-vulnerabilities-code-execution), [Part III: Data Exfiltration](https://www.trendmicro.com/vinfo/us/security/news/threat-landscape/unveiling-ai-agent-vulnerabilities-part-iii-data-exfiltration), and [Part IV: Database Access](https://www.trendmicro.com/vinfo/us/security/news/vulnerabilities-and-exploits/unveiling-ai-agent-vulnerabilities-part-iv-database-access-vulnerabilities).

📌 Malicious AI Models Undermine Software Supply-Chain Security — https://cacm.acm.org/research/malicious-ai-models-undermine-software-supply-chain-security/

📌 Leaking Secrets in the Age of AI —  Shay Berkovich and Rami McCarthy scanned public repos and found widespread AI-related secret leaks — driven by notebooks, hardcoded configs, and gaps in today’s secret scanning tools. https://www.wiz.io/blog/leaking-ai-secrets-in-public-code

📌 What is AI Assets Sprawl? Causes, Risks, and Control Strategies —  Dor Sarig from Pillar Security explores how unmanaged AI models, prompts, and tools accumulate across enterprises — creating security, compliance, and visibility challenges without proper controls. https://www.pillar.security/blog/what-is-ai-assets-sprawl-causes-risks-and-control-strategies   

📌 Is your AI safe? Threat analysis of MCP —  Nil Ashkenazi outlines how Model Context Protocol (MCP) introduces risks like tool misuse, prompt-based exfiltration, and unsafe server chaining. Focus is on real-world attack paths and how insecure integrations can be exploited. https://www.cyberark.com/resources/threat-research-blog/is-your-ai-safe-threat-analysis-of-mcp-model-context-protocol

📌 A New Identity Framework for AI Agents  by Omar Santos — We are all experiencing the rapid proliferation of autonomous AI agents and Multi-Agent Systems (MAS). These are no longer AI chatbots and assistants; they are increasingly self-directed entities capable of making decisions, performing actions, and interacting with critical systems at unprecedented scales. We need to perform fundamental re-evaluations of how identities are managed and access is controlled for these AI agents. https://community.cisco.com/t5/security-blogs/a-new-identity-framework-for-ai-agents/ba-p/5294337

📌 Hunting Deserialization Vulnerabilities With Claude —  TrustedSec exploring how to find zero-days in .NET assemblies using Model Context Protocol (MCP). https://trustedsec.com/blog/hunting-deserialization-vulnerabilities-with-claude

📌 Uncovering Nytheon AI — Vitaly Simonovich 🇮🇱 from Cato Networks, analyzes Nytheon AI, a Tor-based GenAI platform built from jailbroken open-source models (Llama 3.2, Gemma, Qwen2), offering code generation, multilingual chat, image parsing, and API access — wrapped in a modern SaaS-style interface. https://www.catonetworks.com/blog/cato-ctrl-nytheon-ai-a-new-platform-of-uncensored-llms/

📌 Touchpoints Between AI and Non-Human Identities —  Tal Skverer from Astrix Security and Ophir Oren 🇮🇱 from Bayer examine how AI agents rely on non-human identities (NHIs) — API keys, service accounts, OAuth apps — to operate across platforms. Unlike traditional automation, these agents request dynamic access, mimic users, and often require multiple NHIs per task, creating complex, opaque identity chains. https://astrix.security/learn/blog/astrix-research-presents-touchpoints-between-ai-and-non-human-identities/

📌 Breaking down ‘EchoLeak’, Vulnerability Enabling Data Exfiltration from Microsoft 365 Copilot —  Itay Ravia and other members of Aim Security identified a vulnerability in Microsoft 365 Copilot where specially crafted emails can trigger data leakage through prompt injection and markdown/CSP bypasses. The issue stems from how Copilot processes untrusted input, potentially exposing internal content. https://www.aim.security/lp/aim-labs-echoleak-blogpost

📌 Remote Prompt Injection in GitLab Duo Leads to Source Code Theft —  Legit Security’s Omer Mayraz demonstrates how a single hidden comment could trigger GitLab Duo (Claude-powered) to leak private source code, suggest malicious packages, and exfiltrate zero-days. The exploit chain combines prompt injection, invisible text, markdown-to-HTML rendering abuse, and access to sensitive content — showcasing the real-world risks of deeply integrated AI agents in developer workflows. https://www.legitsecurity.com/blog/remote-prompt-injection-in-gitlab-duo

📌 ISO/IEC 42005:2025 has been formally published. ISO 42005 provides guidance for organizations conducting AI system impact assessments. Establishing a process and performing an AI system impact assessment is integral for organizations looking to pursue ISO 42001 certification. More importantly, the AI impact assessment allows organizations to identify high risk AI systems and determine any potential impact to individuals, groups, or societies as it relates to fairness, safety, and transparency. https://www.ethos-ai.org/p/ai-impact-checklist or https://www.linkedin.com/posts/noureddine-kanzari-a852a6181_iso-42005-the-standard-of-the-future-activity-7334498579710943233-ts6S

📌 Checklist for LLM Compliance in Government — Deploying AI in government? Compliance isn’t optional. Missteps can lead to fines reaching $38.5M under global regulations like the EU AI Act — or worse, erode public trust. This checklist ensures your government agency avoids pitfalls and meets ethical standards while deploying large language models (LLMs). https://www.newline.co/@zaoyang/checklist-for-llm-compliance-in-government--1bf1bfd0

📌 How I used o3 to find CVE-2025–37899, a remote zeroday vulnerability in the Linux kernel’s SMB implementation by Sean Heelan. https://sean.heelan.io/2025/05/22/how-i-used-o3-to-find-cve-2025-37899-a-remote-zeroday-vulnerability-in-the-linux-kernels-smb-implementation/

