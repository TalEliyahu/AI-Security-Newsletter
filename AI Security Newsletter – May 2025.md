# Adversarial AI Digest â€“ 20 May, 2025

A digest of AI security research, insights, reports, upcoming events, and tools & resources. Follow the [AI Security community](https://linktr.ee/AISECHUB) on [Twitter](https://twitter.com) and [LinkedIn group](https://linkedin.com) for additional updates.

Sponsored by [CyberBiz](https://cyber-biz.com/) and [InnovGuard.com](https://innovguard.com) â€“ Technology Risk & Cybersecurity Advisory, Innovate and Invest with Confidence, Lead with Assurance.

## ğŸ” Insights
ğŸ“Œ [AI Agents vs. Agentic AI â€” Design Is Defense](https://medium.com/ai-security-hub/ai-agents-vs-agentic-ai-ecfc5d8f41b6) â€” How AI Agents and Agentic AI really differ â€” and why security needs to be part of the design, not an afterthought.

ğŸ“Œ [MITRE ATT&CK Adds Sub-Technique T1588.007: Adversaries Leveraging AI](https://attack.mitre.org/techniques/T1588/007/) â€” ATT&CK v17 introduces a new sub-technique to document how adversaries use LLMs for phishing, payload automation, reconnaissance, and social engineering.

ğŸ“Œ [Agentic AI Security: Key Threats, Attacks, and Defenses](https://adversa.ai/blog/agentic-ai-security/) - Agentic AI systems bring autonomy, memory, and tool useâ€”introducing novel risks like memory injection, tool poisoning, and multi-agent deception. Alex Polyakov at Adversa AI explores why traditional GenAI defenses fall short and how red teaming, sandboxing, and permission controls are essential for securing next-gen autonomous agents.

ğŸ“Œ [AI-Powered Mystery Box Scams](https://infosecwriteups.com/ai-powered-mystery-box-scams-02e931065a19) â€” These scams have flooded social platforms, using AI-generated ads, fake influencers, chatbot support, deepfake visuals, and autogenerated reviews to scale deception.

ğŸ“Œ [Xanthorox AI](https://medium.com/ai-security-hub/xanthorox-ai-84c9d22d3d48) - Xanthorox AI is a self-hosted, multi-model AI platform emerging in cybercrime communities. Xanthorox is built from the ground up to operate entirely offline, avoiding dependence on commercial APIs or cloud infrastructure.

ğŸ“Œ [Nytheon AI](https://medium.com/ai-security-hub/nytheon-ai-1cb2cbc069ec) â€” A dark web GenAI tool with multi-language, real-time malware generation, phishing campaign orchestration, OCR, code writing, and cross-modal capabilities.

ğŸ“Œ [Palo Alto Networks Unit 42 on Synthetic Identity Creation](https://unit42.paloaltonetworks.com/north-korean-synthetic-identity-creation/) â€” North Korean IT workers are using real-time deepfakes to obtain remote jobs and infiltrate organizations.

ğŸ“Œ [How I Used AI to Create a Working Exploit for CVE-2025-32433 by Matthew Keeley](https://platformsecurity.com/blog/CVE-2025-32433-poc) â€” GPT-4 identified the vulnerability, compared the patch, and generated a working PoC.

ğŸ“Œ [Darcula â€” GenAI-Powered Phishing Kit](https://medium.com/ai-security-hub/darcula-genai-powered-phishing-kit-d5c22998c660) - The new version of Darcula, expands its phishing-as-a-service (PhaaS) platform with GenAI-powered phishing kit creation, multi-language templates, customizable forms, and no coding required.

ğŸ“Œ [AI Slop Is Polluting Bug Bounty Platforms](https://socket.dev/blog/ai-slop-polluting-bug-bounty-platforms) â€” Sarah Gooding explores how AI-generated fake vuln reports are wasting triage cycles and damaging trust.

ğŸ“Œ [Everything Wrong with MCP](https://blog.sshh.io/p/everything-wrong-with-mcp) â€” Shrivu Shankar details key flaws in the Model Context Protocol, including trust boundaries, cost control limitations, and prompt injection exposure.

ğŸ“Œ [How to Pitch at RSA Conference Innovation Sandbox, Black Hat Startup Spotlight, and GISEC GLOBAL Cyberstars](https://www.linkedin.com/pulse/how-win-cybersecurity-oscar-tal-eliyahu-aobec/) â€” Cybersecurity startup competitions are more than just pitch stages â€” theyâ€™re credibility engines. This article breaks down what judges actually look for, how AI security is reshaping the field, and why early-stage founders should treat these competitions like investor-level opportunities.

## ğŸ“„ Reports

ğŸ“˜ [AI Agent Governance: A Field Guide](https://www.linkedin.com/feed/update/urn:li:activity:7328876049524543489) â€” A deep dive into agent deployment risks and governance gaps, mapping scenarios, and introducing frameworks for oversight.

ğŸ“˜ [Agent Name Service (ANS) for Secure AI Agent Discovery](https://www.linkedin.com/feed/update/urn:li:activity:7328823746142568449) â€” Introduces a secure, DNS-inspired framework for AI agent discovery using PKI, structured schemas, and protocol adapters (A2A, MCP, ACP).

ğŸ“˜ [IDC: AI in Security Ops](https://www.linkedin.com/feed/update/urn:li:activity:7325564352156119040) â€” A survey of 900+ professionals highlighting AI use cases, deployment frictions, and the need for governance frameworks.

ğŸ“˜ [Securing AI: Addressing the OWASP Top 10 for LLM Applications](https://www.icitech.org/post/securing-aiaddressing-the-owasp-top-10-forlarge-language-model-applications) â€” By Dr. Darren Death at ICIT.

ğŸ“˜ [Cobalt & Cyentia Institute: State of Pentesting 2025](https://www.linkedin.com/feed/update/urn:li:activity:7324377543984189440) â€” Highlights AI-specific penetration testing trends, fix rates, and vulnerabilities in LLMs.

ğŸ“˜ [Check Point Software: State of AI Cybersecurity 2025](https://www.linkedin.com/feed/update/urn:li:activity:7323441087509839872) â€” A detailed look at GenAI use in phishing, malware, voice deepfakes, and criminal toolkits.

ğŸ“˜ [Cisco: State of AI Security](https://www.linkedin.com/feed/update/urn:li:activity:7319411865632010241) â€” Covers infrastructure threats, AI policy trends, and attack vectors like prompt injection.

ğŸ“˜ [Resemble AI: Deepfake Threats Q1 2025](https://www.resemble.ai/q1-2025-ai-deepfake-security-report/) â€” Quantifies deepfake-driven fraud, identity risks, and emerging multimodal threats.

ğŸ“˜ [IBM Security X-Force: GenAI Threat Highlights](https://www.linkedin.com/feed/update/urn:li:activity:7319100078298648576) â€” Breaks down GenAI misuse trends and the gap between adoption and risk ownership.

ğŸ“˜ [Imperva: Bad Bot Report](https://www.linkedin.com/feed/update/urn:li:activity:7318788439192006657) â€” Explores AI bot dominance in web traffic and major sources of automation.

ğŸ“˜ [ETSI SAI Baseline Security for AI Systems](https://www.etsi.org/deliver/etsi_ts/104200_104299/104223/01.01.01_60/ts_104223v010101p.pdf) â€” Introduces baseline cybersecurity requirements and verification guidance.

ğŸ“˜ [Automating Deception: AI in Romance Fraud](https://www.linkedin.com/feed/update/urn:li:activity:7318016094340857856) â€” Simon Moseley explores the role of GenAI in industrializing romance fraud at scale.

ğŸ“˜ [AI Privacy Risks in LLMs - EDPB](https://www.linkedin.com/feed/update/urn:li:activity:7318291536704598016) â€” Isabel BarberÃ¡ analyzes data leakage, prompt injection, and weak access controls across the LLM lifecycle.

ğŸ“˜ [Microsoft: Taxonomy of Failure Modes in Agentic AI](https://www.linkedin.com/feed/update/urn:li:activity:7323553744065900547) â€” Categorizes failures, with memory corruption case studies and red team insights.

---

## ğŸ“‚ Upcoming Events

ğŸ“… [The AI Summit at Black Hat USA â€” August 5, 2025](https://www.blackhat.com/us-25/ai-summit.html) | Mandalay Bay, Las Vegas

ğŸ“… [Artificial Intelligence Risk Summit â€” August 19â€“20, 2025](https://www.airisksummit.com/)

ğŸ“… [The AI Summit at Security Education Conference Toronto (SecTor) 2025 â€” September 30, 2025](https://www.blackhat.com/sector/2025/ai-summit.html) | MTCC, Toronto, Ontario, Canada

ğŸ“… [AI and Security: Transforming Modern AppDev by Techstrong Learning â€” June 4, 2025](https://www.techstrongevents.com/virtual-ai-and-security)

ğŸ“… [The International Conference on Cybersecurity and AI-Based Systems â€” September 1â€“4, 2025](https://www.cyber-ai.org/) | Bulgaria

---

## ğŸ“š Research

ğŸ“– [Understanding LLM Supply Chains](https://arxiv.org/abs/2504.20763) â€” A deep mapping of 15,000+ open-source packages reveals how a small number of libraries (like transformers, langchain) dominate the ecosystem. A single vulnerability in one core package can indirectly affect 1,000+ others via transitive dependencies. Most issues go unreported through formal CVEs.

ğŸ“– [Full-Stack Safety Survey for LLMs](https://arxiv.org/abs/2504.15585) â€” The first end-to-end review of safety challenges across the LLM lifecycle â€” from data collection to commercialization. Synthesizes over 800 papers, highlights risks in alignment, deployment, and model editing, and proposes forward-looking safeguards.

ğŸ“– [The Leaderboard Illusion](https://arxiv.org/pdf/2504.20879) â€” A multi-institutional audit of Chatbot Arena exposes manipulation risks, duplicated prompts, silent model removal, and data inequality. Open-weight models receive less training data and sampling attention, distorting perception of model quality.

ğŸ“– [LLM Agent Privacy & Security Survey](https://arxiv.org/html/2407.19354v1) â€” Categorizes nine key vulnerabilities in LLM agents, from hallucinations and knowledge poisoning to data leakage and agent manipulation. Case studies reveal the real-world impacts and gaps in current defenses.

ğŸ“– [Control Levels for LLM Agents](https://arxiv.org/abs/2504.05259) â€” Introduces a 5-level framework (ACLs 1â€“5) to align security measures with AI agent capabilities. Offers structured control evaluation rules and red teaming affordances for increasingly powerful agents.

ğŸ“– [Package Hallucination in LLMs](https://arxiv.org/abs/2406.10279) â€” LLMs frequently invent nonexistent libraries (e.g., `pip install xyz`) â€” a risk now named slopsquatting. Adversaries can register these fake packages with malware. Study shows 19.7% hallucination rate across 16 LLMs and 576K samples.

ğŸ“– [Enterprise-Grade MCP Security](https://arxiv.org/abs/2504.08623) â€” AWS-backed paper on securing AI toolchains using Model Context Protocol. Recommends sandboxing, zero-trust design, and per-request access policies to mitigate tool poisoning, data leaks, and exfiltration.

ğŸ“– [Can LLMs Classify CVEs?](https://arxiv.org/pdf/2504.10713v1) â€” Proposes a hybrid CVSS scoring pipeline combining Gemma 3 for objective fields with MiniLM+XGBoost for subjective ones, achieving 84% accuracy.

ğŸ“– [Open Problems in Technical AI Governance](https://arxiv.org/abs/2407.14981) â€” Maps unresolved challenges in verifying, monitoring, and securing AI systems â€” from data traceability to model access policies and third-party audits.

ğŸ“– [RAG LLMs Are Not Safer](https://arxiv.org/pdf/2504.18041) â€” Retrieval-augmented generation changes model behavior and risk profile. Even safe docs + safe models can yield unsafe outputs. Study shows red teaming methods are less effective in RAG settings.

---

## ğŸ›  Tools & Resources

ğŸ§° [Prompt Hacking Resources](https://github.com/PromptLabs/Prompt-Hacking-Resources) â€” A curated repo of jailbreaks, model attacks, and defense methods.

ğŸ§° [OWASP AISVS](https://github.com/OWASP/AISVS) â€” A verification standard for AI application security, akin to ASVS for traditional apps.

ğŸ§° [MCP Security Checklist by SlowMist](https://github.com/slowmist/MCP-Security-Checklist) â€” Audit-ready checklist for securing Model Context Protocol deployments.

ğŸ§° [Vulnerable MCP Project](https://vulnerablemcp.info/) â€” A live registry of known vulnerabilities in Model Context Protocol implementations.

ğŸ§° [MCP for Security Tools by Cyprox](https://github.com/cyproxio/mcp-for-security) â€” Integrate MCP with tools like Nmap, FFUF, SQLMap, and more.

ğŸ§° [Aetheris AI SBOM Generator by Capyx](https://huggingface.co/spaces/aetheris-ai/aibom-generator) â€” Extract and format model metadata and dependencies into CycloneDX SBOMs.

ğŸ§° [Synthetic Data Kit by Meta](https://github.com/meta-llama/synthetic-data-kit) â€” Generate synthetic datasets for LLM tuning.

ğŸ§° [NOVA Prompt Security Tool by Thomas Roccia](https://github.com/fr0gger/nova-framework) | [NovaHunting.ai](https://novahunting.ai/) â€” Hunt adversarial prompts, jailbreaks, and prompt injections.

ğŸ§° [Tencent AI-Infra-Guard](https://github.com/Tencent/AI-Infra-Guard) â€” Lightweight tool for AI infrastructure vulnerability scanning.

ğŸ§° [Taranis AI](https://github.com/taranis-ai/taranis-ai) â€” OSINT-focused AI platform for situational awareness and intel collection.

---

## ğŸ¥ Videos

â–¶ï¸ [Data-Driven Insights into AI and Cybersecurity - Sandra Joyce & Mohamed Alkuwaiti](https://www.youtube.com/watch?v=IAhU1AAqxcY)

â–¶ï¸ [Backdooring AI Models - SANS Institute, Ahmed AbuGharbia](https://www.youtube.com/watch?v=orsGzJgU6Eo)

â–¶ï¸ [I Backdoored Cursor AI - John Hammond](https://youtu.be/FYok3diZY78)

â–¶ï¸ [Finding Web App Vulnerabilities with AI](https://youtu.be/v-McepNOrTQ)

â–¶ï¸ [The Art of Poison-Pilling Music Files - Benn Jordan](https://www.youtube.com/watch?v=xMYm2d9bmEA)

â–¶ï¸ [A2A - MCP SECURITY Threats: Protect your AI Agents](https://www.youtube.com/watch?v=h_6unQxHyb4)

â–¶ï¸ [What is LLMJacking? The Hidden Cloud Security Threat of AI Models - Jeff Crume](https://www.youtube.com/watch?v=dibZ1itSvM4)

â–¶ï¸ [How to Attack and Defend LLMs: AI Security Explained - Yaniv Hoffman](https://www.youtube.com/watch?v=6bYGhY9HB8k)

â–¶ï¸ [SpAIware & More: Advanced Prompt Injection Exploits in LLM Applications - Johann Rehberger](https://www.youtube.com/watch?v=84NVG1c5LRI)

---
